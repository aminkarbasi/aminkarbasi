<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3548987469263964"
     crossorigin="anonymous"></script>

    <meta name="author" content="Inference, Information,  and Decision Systems Group">
    <meta name="description" content="">
    <meta name="keywords" content="blog,developer,personal">

    <meta property="og:site_name" content="IID Group">
    <meta property="og:title" content="
  ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back - IID Group
">
    <meta property="og:description" content="about">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/icml/icml-20.md/">
    <meta property="og:image" content="images/tn.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="/icml/icml-20.md/">
    <meta name="twitter:image" content="images/tn.png">

    <base href="/icml/icml-20.md/">
    <title>
  ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back - IID Group
</title>

    <link rel="canonical" href="/icml/icml-20.md/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="/index.xml" type="application/rss+xml" title="IID Group">
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="IID Group" />
    

    <meta name="generator" content="Hugo 0.115.0">
  </head>

  <body class="">
    <main class="wrapper">
      
<link href="https://fonts.googleapis.com/css?family=Baloo+Chettan+2&display=swap" rel="stylesheet">

<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.dropbtn {
  background-color:  #00356B;
  color: white;
  padding: 8px;
  font-size: 16px;
  border: none;
  border-radius:1px; 
}

.dropdown {
  position: relative;
  display: inline-block;
  padding:4px;
}

.dropdown-content {
  display: none;
  position: absolute;
  background-color: #f1f1f1;
  min-width: 180px;
  box-shadow: 0px 0px 0px 0px rgba(0,0,0,0.2);
  margin-left:0px;
  z-index: 1;
}

.dropdown-content a {
  color: black;
  padding: 0px;
  text-decoration: none;
  display: block;
  margin-left:0px;
  margin-right:2px;
}

.dropdown-content a:hover {background-color: #ddd;}

.dropdown:hover .dropdown-content {display: block;}

.dropdown:hover .dropbtn {background-color: #3e8e41;}
</style>
</head>

<nav class="navigation">
  <section class="container">
	  <span class="navigation-title" href="/">
		  <img src="/images/logos/iid.png"/></a><span class="navigation-text">&nbsp;IID Group</span></span>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
		
			
              		<a class="navigation-link" href="/">About</a>
			
		
            </li>
          
            <li class="navigation-item  align-center ">
		
			
              		<a class="navigation-link" href="/people">People</a>
			
		
            </li>
          
            <li class="navigation-item  align-center ">
		
			
              		<a class="navigation-link" href="/publications/">Publications</a>
			
		
            </li>
          
            <li class="navigation-item  align-center ">
		
			
              		<a class="navigation-link" href="/research">Research</a>
			
		
            </li>
          
            <li class="navigation-item  align-center ">
		
			
              		<a class="navigation-link" href="/courses">Teaching</a>
			
		
            </li>
          
            <li class="navigation-item  align-center ">
		 
		<ul class="dropdown">
			<a class="dropbtn" style="line-height:5rem;" href="#">other</a>
		        <ul class="dropdown-content">
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/openings/openings.md">Openings</a>
			    </li>
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/hunala/hunala.md">Hunala </a>
			    </li>
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/software/software.md">Software</a>
			    </li>
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/outreach/outreach.md">Outreach</a>
			    </li>
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/contact/contact.md">Contact</a>
			    </li>
		  
			    <li class="align-center">
			      <a style="margin-left:-1rem;" href="/icml/icml-20.md">Tutorial</a>
			    </li>
		  
		    </ul>
		</ul>

		
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  
<section class="container page">
	<article>
		<header>
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
			
			<div></div>
			<h1 >Title:&nbsp;ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</h1>
			<h1 >Speakers:&nbsp;Hamed Hassani (UPenn), Amin Karbasi (Yale)</h1>
			<div>
			

		</header>

		<div  class="x-column x-sm x-1-6" style=""  >
		</div>

			<script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>
			<div>
			
			<p><img src="/images/icml-2020/Hassani.jpg" alt="math fest 1" title="hassani">
<img src="/images/icml-2020/karbasi.jpeg" alt="math fest 1" title="karbasi"></p>
<ol>
<li><a href="https://www.seas.upenn.edu/~hassani/">Hamed Hassani</a></li>
</ol>
<blockquote>
<p>School of Engineering and Applied Sciences<br>
University of Pennsylvania<br>
Philadelphia, PA 19104</p>
</blockquote>
<ol start="2">
<li><a href="http://iid.yale.edu/people/amin-karbasi/">Amin Karbasi</a></li>
</ol>
<blockquote>
<p>Yale Institute for Network Science <br>
Yale University <br>
New Haven, CT 06520</p>
</blockquote>
<h1 id="slides">Slides</h1>
<ol>
<li><a href="/files/icml-2020/module1.pdf">Module 1</a></li>
<li><a href="/files/icml-2020/module2.pdf">Module 2</a></li>
<li><a href="/files/icml-2020/module3.pdf">Module 3</a></li>
<li><a href="/files/icml-2020/module4.pdf">Module 4</a></li>
</ol>
<h1 id="videos">Videos</h1>
<ol>
<li><a href="https://slideslive.com/38930508/submodular-optimization-from-discrete-to-continuous-and-back-part-i?ref=account-folder-55826-folders">Part I</a></li>
<li><a href="https://slideslive.com/38930509/submodular-optimization-from-discrete-to-continuous-and-back-part-ii?ref=account-folder-55826-folders">Part II</a></li>
<li><a href="https://slideslive.com/38930506/submodular-optimization-from-discrete-to-continuous-and-back-part-iii?ref=account-folder-55826-folders">Part III</a></li>
<li><a href="https://slideslive.com/38930507/submodular-optimization-from-discrete-to-continuous-and-back-part-iv?ref=account-folder-55826-folders">Part IV</a></li>
</ol>
<h1 id="brief-description-and-outline">Brief Description and Outline</h1>
<p>This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design
fast algorithms with provable guarantees for a broad range of applications. In recent years, however,
there has been a surge of interest in applications that involve discrete optimization. For discrete
domains, the analog of convexity is considered to be submodularity, and the evolving theory of
submodular optimization has been a catalyst for progress in extraordinarily varied application
areas including active learning and experimental design, vision, sparse reconstruction, graph inference, video analysis, clustering, document summarization, object detection, information retrieval,
network inference, interpreting neural network, and discrete adversarial attacks.</p>
<p>              <img src="/images/icml-2020/submodular.png" alt="math fest 1" title="submodular"></p>
<p>As applications and techniques of submodular optimization mature, a fundamental gap between
theory and application emerges. In the past decade, paradigms such as large-scale learning, distributed systems, and sequential decision making have enabled a quantum leap in the performance
of learning methodologies. Incorporating these paradigms in discrete problems has led to fundamentally new frameworks for submodular optimization. The goal of this tutorial is to cover rigorous
and scalable foundations for discrete optimization in complex, dynamic environments, addressing
challenges of scalability and uncertainty, and facilitating distributed and sequential learning in
broader discrete settings. More specifically, we will cover advancements in four areas:</p>
<ol>
<li>
<h2 id="submodular-optimization-with-perfect-information">Submodular Optimization with Perfect Information.</h2>
</li>
</ol>
<p>State-of-the-art submodular optimization techniques have mostly assumed perfect knowledge about the optimization problem
and the environment. More precisely, the existing techniques have been designed based on
the availability of an oracle with full information about the objective function at any queried
point. In this context, recent work in submodular optimization has widely focused on developing fast, efficient, and provable methodologies in the presence of massive, high-dimensional
data:</p>
<ul>
<li>
<p>Fast Greedy Methods. The first class of methods consider cases where the size and
dimension of the data is large, but data can be fully accessible and stored in memory.
Accordingly, fast greedy algorithms have been proposed recently with almost optimal
run-time efficiency in terms of the query complexity and dimension of data while maintaining optimal quality guarantees in terms of objective value <a href="/icml/icml-20.md/#1">1, 2, 3, 4,49, 50-61</a>.</p>
</li>
<li>
<p>Streaming Algorithms: In cases where the data sets are too large to be stored in
memory, we consider streaming techniques, where computation is performed on the
y.
A recent line of work has been focused on developing algorithms that obtain desirable
guarantees for important tasks in machine learning while using modest memory resources
<a href="/icml/icml-20.md/#5">5, 6, 7, 8, 9,62-68</a>.</p>
</li>
<li>
<p>Distributed Optimization: The rise of distributed computation frameworks such
as MapReduce, Hadoop, and Spark, enabled unprecedented advancement in machine
1
learning. Although submodular optimization is a canonical example of computation that
cannot be parallelized, there is a great deal of work on new algorithm design approaches
that can utilize distributed computational resources <a href="/icml/icml-20.md/#10">10, 11, 12, 13, 14, 15, 16,</a><a href="/icml/icml-20.md/#69">69-77</a>.</p>
</li>
</ul>
<ol start="2">
<li>
<h2 id="submodular-optimization-with-imperfect-information">Submodular Optimization with Imperfect Information</h2>
</li>
</ol>
<p>In many modern applications, perfect-information oracles can not assumed, and instead, we resort to approximate or
imperfect ones in order to address challenges such as scalability, uncertainty in data and models, as well as dynamic and adversarial environments. We will formalize oracles with imperfect
information through three main classes{Stochastic, Online, and Bandit} each of which opens
a new avenue in the field of discrete (submodular) optimization and requires fundamentally
novel techniques:</p>
<ul>
<li>
<p>The Stochastic Oracle. The stochastic oracle returns a stochastic, but unbiased estimate of the function value at any query point. Given this type of oracle access to the
function values, the primary question is to develop stochastic submodular optimization
techniques with minimal sample complexity (i.e. the total number of queries from the
stochastic oracle) and computation complexity <a href="/icml/icml-20.md/#17">17, 18</a>. The stochastic oracle is motivated form practical and large-scale applications and covers popular instances of discrete
optimization: (i) Oftentimes in practice the objective is defined in terms of an empirical
risk, i.e., through a finite sum of loss functions associated to the data points. This formulation appears in submodular applications such as data summarization <a href="/icml/icml-20.md/#19">19, 20, 21</a>,
recommender systems <a href="/icml/icml-20.md/#22">22, 18</a>, and sparse regression <a href="/icml/icml-20.md/#23">23, 24, 25</a> etc. (ii) In some other
cases, the objective is given as an expectation over an explicit stochastic model which is
hard/intractable to compute. For example, in influence maximization in social networks
<a href="/icml/icml-20.md/#26">26</a>,the objective value is defned as the expectation of a stochastic difiusion process,
quantifying the expected fraction of the network influenced from a selected seed set.</p>
</li>
<li>
<p>Online/Bandit Oracle. This type of oracle is the model of choice when the optimizer/learner has to interact with an unknown, evolving, or even adversarially changing
environment. The oracle&rsquo;s outcome in this setting varies with time, and depending on
how limited the oracle&rsquo;s outcome is, we can define two different scenarios: online, and
bandit settings. The primary optimization goal in this setting is to provide no-regret
mechanisms to maximize the cumulative objective over time. Online and Bandit submodular optimization, introduced in <a href="/icml/icml-20.md/#27">27, 28, 29</a>, arises naturally in discrete applications
involving dynamic environments, e.g. online/dynamic resource allocation <a href="/icml/icml-20.md/#23">30, 27</a>, online
ranking <a href="/icml/icml-20.md/#31">31, 32</a>, sensor placement in unknown or adversarial environments <a href="/icml/icml-20.md/#33">33</a>, in
uencing dynamic social networks <a href="/icml/icml-20.md/#34">34</a>, online recommender systems <a href="/icml/icml-20.md/#35">35, 36</a>, and dynamic
multi-robot coverage problems <a href="/icml/icml-20.md/#37">37</a>.</p>
</li>
</ul>
<!-- raw HTML omitted -->
<ol start="3">
<li>
<h2 id="a-non-convex-bridge-between-discrete-and-continuous-optimization">A Non-Convex Bridge Between Discrete and Continuous Optimization</h2>
</li>
</ol>
<p>To address
discrete otimization with imperfect oracles, we will describe a recently developed framework
which bridges discrete and continuous optimization. Through appropriate continuous extensions, this framework devises algorithms that succeed in finding good solutions by making
a large number of small, exploratory steps using noisy estimates of the objective value, and
moving toward the optimal solution. As we will discuss, solving the resulting continuous problems requires novel methods beyond the state-of-the-art, because they are highly non-convex
and possess undesirable stationary points. We show how tools from discrete and continuous
optimization as well as techniques that exploit the special structure (submodularity) admitted
by these problems and avoid those bad stationary points <a href="/icml/icml-20.md/#18">18</a> <a href="/icml/icml-20.md/#38">38, 39, 40, 41, 42, 43</a>.
Indeed, submodular optimization is inherently related to continuous optimization as many
submodular optimization methods rely on continuous relaxations. This connection has recently been strengthen by introducing continuous submodular functions <a href="/icml/icml-20.md/#44">44, 45, 78</a>, <a href="/icml/icml-20.md/#18">18</a>. Such
functions are not convex (nor concave) but still allow finding near-optimal solutions, thus
providing a rich framework for studying non-convex optimization problems.</p>
<ol start="4">
<li>
<h2 id="beyond-submodularity">Beyond Submodularity</h2>
</li>
</ol>
<p>Finally, one can generalize the notion of submodularity and still
provide approximation guarantees. Several of such generalizations, e.g., adaptive submodularity <a href="/icml/icml-20.md/#46">46, 47</a>, weak submodularity <a href="/icml/icml-20.md/#25">25</a>, <a href="/icml/icml-20.md/#9">9</a>, two-stage submodularity <a href="/icml/icml-20.md/#48">48</a>, etc, have been
recently proposed. These generalizations extend the applications of submodularity to interactive settings, dictionary learning, and sparse recovery.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

			

			<div></div>
			</ul>
			</div>

	</article>
</section>





<section class="container list">
	<ul>
		
		

		
		<h1>References</h1>
		<table>
			
		</table>
	</ul>
</section>




      </div>
      
        <footer class="footer">
  <section class="container">
   <div></div>
    
      <div class="sns-shares sp-sns-shares">
        
          <a class="sns-share twitter-share" href="https://twitter.com/intent/tweet?original_referer=%2ficml%2ficml-20.md%2f&ref_src=twsrc%5Etfw&text=ICML%202020%20Tutorial%20on%20Submodular%20Optimization%3a%20From%20Discrete%20to%20Continuous%20and%20Back IID%20Group&tw_p=tweetbutton&url=%2ficml%2ficml-20.md%2f"><i class="fab fa-twitter"></i></a>
        
        
          <a class="fb btn sns-share fb-share" href="http://www.facebook.com/share.php?u=%2ficml%2ficml-20.md%2f" onclick="window.open(this.href, 'FBwindow', 'width=650, height=450, menubar=no, toolbar=no, scrollbars=yes'); return false;"><i class="fab fa-facebook-f"></i></a>
        
        
          <a class="sns-share line-share" href="https://social-plugins.line.me/lineit/share?url=%2ficml%2ficml-20.md%2f"><i class="fab fa-line"></i></a>
        
        
          <a class="sns-share linkedIn-share" href="https://www.linkedin.com/sharing/share-offsite/?url=%2ficml%2ficml-20.md%2f"><i class="fab fa-linkedin"></i></a>
        
      </div>
    
    
    
      <p>Yale University</p>
    

  </section>
</footer>
<style>
#extreme
{
z-index: 1;
visibility: hidden;
position: absolute;
z-index: 10000;
}
</style>
<div id=”extreme”>
    <script style="width:28px;height:28px" src="//t1.extreme-dm.com/f.js" id="eXF-karbasi-0" async defer></script>
</div>
<div class="fixed-bar">
  <section class="container">
    
    <p id="privateTriggerText" style="color:white"><a style="color:white" href="https://usability.yale.edu/web-accessibility/accessibility-yale">Accessibility at Yale</a> &nbsp; Inference, Information, and Decision Group at Yale </p>
    
  </section>
</div>

      
    </main>

    

  <script src="js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("Thank You! Please share it if you like it→");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
