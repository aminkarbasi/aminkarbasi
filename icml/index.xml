<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Icmls on IID Group</title>
    <link>/icml/</link>
    <description>Recent content in Icmls on IID Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 30 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="/icml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</title>
      <link>/icml/icml-20.md/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/icml-20.md/</guid>
      <description>Hamed Hassani School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104
Amin Karbasi Yale Institute for Network Science Yale University New Haven, CT 06520
Slides Module 1 Module 2 Module 3 Module 4 Videos Part I Part II Part III Part IV Brief Description and Outline This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications.</description>
    </item>
    
  </channel>
</rss>
