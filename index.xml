<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IID Group</title>
    <link>/</link>
    <description>Recent content on IID Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 27 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cross Atlas Remapping via Optimal Transport (CAROT): Creating connectomes for different atlases when raw data is not available</title>
      <link>/publications/2023/media/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/media/</guid>
      <description>Abstract Open-source, publicly available neuroimaging datasets—whether from large-scale data collection efforts or pooled from multiple smaller studies—offer unprecedented sample sizes and promote generalization efforts. Releasing data can democratize science, increase the replicability of findings, and lead to discoveries. Due to patient privacy and data storage concerns, researchers typically release preprocessed data with the voxelwise time series parcellated into a map of predefined regions, known as an atlas. However, releasing preprocessed data also limits the choices available to the end-user.</description>
    </item>
    
    <item>
      <title>Data-driven mapping between functional connectomes using optimal transport</title>
      <link>/publications/2021/javid-2021/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/javid-2021/</guid>
      <description>Abstract Functional connectomes derived from functional magnetic resonance imaging have long been used to understand the functional organization of the brain. Nevertheless, a connectome is intrinsically linked to the atlas used to create it. In other words, a connectome generated from one atlas is different in scale and resolution compared to a connectome generated from another atlas. Being able to map connectomes and derived results between different atlases without additional pre-processing is a crucial step in improving interpretation and generalization between studies that use different atlases.</description>
    </item>
    
    <item>
      <title>Batched Neural Bandits</title>
      <link>/publications/2021/gu-2021/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gu-2021/</guid>
      <description>Abstract In many sequential decision-making problems, the individuals are split into several batches and the decision-maker is only allowed to change her policy at the end of batches. These batch problems have a large number of applications, ranging from clinical trials to crowdsourcing. Motivated by this, we study the stochastic contextual bandit problem for general reward distributions under the batched setting. We propose the BatchNeuralUCB algorithm which combines neural networks with optimism to address the exploration-exploitation tradeoff while keeping the total number of batches limited.</description>
    </item>
    
    <item>
      <title>Safe Learning under Uncertain Objectives and Constraints</title>
      <link>/publications/2020/karbasi-2020/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020/</guid>
      <description>Abstract In this paper, we consider non-convex optimization problems under\textit {unknown} yet safety-critical constraints. Such problems naturally arise in a variety of domains including robotics, manufacturing, and medical procedures, where it is infeasible to know or identify all the constraints. Therefore, the parameter space should be explored in a conservative way to ensure that none of the constraints are violated during the optimization process once we start from a safe initialization point.</description>
    </item>
    
    <item>
      <title>Streaming Submodular Maximization under a k-Set System Constraint. </title>
      <link>/info/info/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/info/info/</guid>
      <description>Welcome to the I.I.D. group, led by Amin Karbasi. The research in our group is at the intersection of learning theory[1,2], optimization[3,4], and information processing[5,6]. We develop data-driven algorithms, with strong theoretical guarantees, that can automatically acquire and reason about highly uncertain information. For more information, please visit our research directions.</description>
    </item>
    
    <item>
      <title>Trustworthy ML &amp; Robust Statistics</title>
      <link>/research/trust.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/trust.md/</guid>
      <description>As machine learning systems are pervasively deployed in many scientific fields with increasingly sensitive tasks, it has become paramount to develop algorithms that are robust against the numerous sources of uncertainty inherent in those applications including noise in the data, malicious exploitation of vulnerabilities, outliers, variability of the true objective, privacy, and fairness. While current research in machine learning has led to fundamental breakthroughs, there is still a large gap between the theory and the limitations of the existing algorithms used by practitioners in the real world.</description>
    </item>
    
    <item>
      <title>Batched Multi-Armed Bandits with Optimal Regret</title>
      <link>/publications/2019/esfandiari-2019a/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/esfandiari-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1−1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1−1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1−1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2019/lin-2019d/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Θ(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask.</title>
      <link>/publications/2018/ghili-18a/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/ghili-18a/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Deletion-Robust Submodular Maximization at Scale.</title>
      <link>/publications/2017/kazemi-17a/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/kazemi-17a/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting</title>
      <link>/publications/2016/lin-16d/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16d/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Adaptive Content Search Through Comparisons</title>
      <link>/publications/2011/karbasi-11c/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11c/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner: the user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>An Estimation Theoretic Approach for Sparsity Pattern Recovery in the Noisy Setting</title>
      <link>/publications/2009/hormati-09a/</link>
      <pubDate>Sat, 03 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/hormati-09a/</guid>
      <description>Abstract Compressed sensing deals with the reconstruction of sparse signals using a small number of linear measurements. One of the main challenges in compressed sensing is to find the support of a sparse signal. In the literature, several bounds on the scaling law of the number of measurements for successful support recovery have been derived where the main focus is on random Gaussian measurement matrices. In this paper, we investigate the noisy support recovery problem from an estimation theoretic point of view, where no specific assumption is made on the underlying measurement matrix.</description>
    </item>
    
    <item>
      <title>Accelerating Transformers via Kernel Density Estimation</title>
      <link>/publications/2023/icml-1/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-1/</guid>
      <description>Abstract Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, naïve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products.</description>
    </item>
    
    <item>
      <title>Approximate Thompson Sampling with Logarithmic Batches: Bandits and Reinforcement Learning</title>
      <link>/publications/2023/icml-3/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-3/</guid>
      <description>Abstract TBC</description>
    </item>
    
    <item>
      <title>Statistical Indistinguishability of Learning Algorithms</title>
      <link>/publications/2023/icml-2/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-2/</guid>
      <description>Abstract TBC</description>
    </item>
    
    <item>
      <title>Stacking multiple optimal transport policies to map functional connectomes</title>
      <link>/publications/2023/ciss-20223/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/ciss-20223/</guid>
      <description>Abstract Connectomics is a popular approach for understanding the brain with neuroimaging data. However, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from another. Consequently, connectomes generated from different atlases cannot be used in the same analysis. This limitation hinders efforts toward increasing sample size and demonstrating generalizability across datasets. Recently, we proposed Cross Atlas Remapping via Optimal Transport (CAROT) to find a spatial mapping between a pair of atlases based on a set of training data.</description>
    </item>
    
    <item>
      <title>Exact Gradient Computation for Spiking Neural Networks Through Forward Propagation</title>
      <link>/publications/2023/aistats-2022a/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/aistats-2022a/</guid>
      <description>Abstract Spiking neural networks (SNN) have recently emerged as alternatives to traditional neural networks, owing to energy efficiency benefits and capacity to better capture biological neuronal mechanisms. However, the classic backpropagation algorithm for training traditional networks has been notoriously difficult to apply to SNN due to the hard-thresholding and discontinuities at spike times. Therefore, a large majority of prior work believes exact gradients for SNN w.r.t. their weights do not exist and has focused on approximation methods to produce surrogate gradients.</description>
    </item>
    
    <item>
      <title>Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD</title>
      <link>/publications/2023/iclar-2022b/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/iclar-2022b/</guid>
      <description>Abstract We provide sharp path-dependent generalization and excess risk guarantees for the full-batch Gradient Descent (GD) algorithm on smooth losses (possibly non-Lipschitz, possibly nonconvex). At the heart of our analysis is an upper bound on the generalization error, which implies that average output stability and a bounded expected optimization error at termination lead to generalization. This result shows that a small generalization error occurs along the optimization path, and allows us to bypass Lipschitz or sub-Gaussian assumptions on the loss prevalent in previous works.</description>
    </item>
    
    <item>
      <title>Reproducible Bandits</title>
      <link>/publications/2023/iclar-2022a/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/iclar-2022a/</guid>
      <description>Abstract In this paper, we introduce the notion of reproducible policies in the context of stochastic bandits, one of the canonical problems in interactive learning. A policy in the bandit environment is called reproducible if it pulls, with high probability, the \emph{exact} same sequence of arms in two different and independent executions (i.e., under independent reward realizations). We show that not only do reproducible policies exist, but also they achieve almost the same optimal (non-reproducible) regret bounds in terms of the time horizon.</description>
    </item>
    
    <item>
      <title>Black-Box Generalization</title>
      <link>/publications/2022/neurips-4/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-4/</guid>
      <description>Abstract We provide the first generalization error analysis for black-box learning through derivative-free optimization. Under the assumption of a Lipschitz and smooth unknown loss, we consider the Zeroth-order Stochastic Search (ZoSS) algorithm, that updates a d-dimensional model by replacing stochastic gradient directions with stochastic differences of K+1 perturbed loss evaluations per dataset (example) query. For both unbounded and bounded possibly nonconvex losses, we present the first generalization bounds for the ZoSS algorithm.</description>
    </item>
    
    <item>
      <title>Fast Neural Kernel Embeddings for General Activations</title>
      <link>/publications/2022/neurips-6/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-6/</guid>
      <description>Abstract Infinite width limit has shed light on generalization and optimization aspects of deep learning by establishing connections between neural networks and kernel methods. Despite their importance, the utility of these kernel methods was limited in large-scale learning settings due to their (super-)quadratic runtime and memory complexities. Moreover, most prior works on neural kernels have focused on the ReLU activation, mainly due to its popularity but also due to the difficulty of computing such kernels for general activations.</description>
    </item>
    
    <item>
      <title>Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes </title>
      <link>/publications/2022/neurips-3/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-3/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>On Optimal Learning Under Targeted Data Poisoning </title>
      <link>/publications/2022/neurips-5/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-5/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Submodular Maximization in Clean Linear Time</title>
      <link>/publications/2022/li-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/li-2022/</guid>
      <description>Abstract In this paper, we provide the first deterministic algorithm that achieves the tight 1−1/e approximation guarantee for submodular maximization under a cardinality (size) constraint while making a number of queries that scales only linearly with the size of the ground set n. To complement our result, we also show strong information-theoretic lower bounds. More specifically, we show that when the maximum cardinality allowed for a solution is constant, no algorithm making a sub-linear number of function evaluations can guarantee any constant approximation ratio.</description>
    </item>
    
    <item>
      <title>The Best of Both Worlds: Reinforcement Learning with Logarithmic Regret and Policy Switches</title>
      <link>/publications/2022/icml2-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml2-2022/</guid>
      <description>Abstract In this paper, we study the problem of regret minimization for episodic Reinforcement Learning (RL) both in the model-free and the model-based setting. We focus on learning with general function classes and general model classes, and we derive results that scale with the eluder dimension of these classes. In contrast to the existing body of work that mainly establishes instance-independent regret guarantees, we focus on the instance-dependent setting and show that the regret scales logarithmically with the horizon T, provided that there is a gap between the best and the second best action in every state.</description>
    </item>
    
    <item>
      <title>Universal Rates for Interactive Learning</title>
      <link>/publications/2022/neurips-7/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-7/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Self-Consistency of the Fokker-Planck Equation</title>
      <link>/publications/2022/karbasi-2022b/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022b/</guid>
      <description>Abstract The Fokker-Planck equation (FPE) is the partial differential equation that governs the density evolution of the Itô process and is of great importance to the literature of statistical physics and machine learning. The FPE can be regarded as a continuity equation where the change of the density is completely determined by a time varying velocity field. Importantly, this velocity field also depends on the current density function. As a result, the ground-truth velocity field can be shown to be the solution of a fixed-point equation, a property that we call self-consistency.</description>
    </item>
    
    <item>
      <title>Scalable MCMC Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/icml3-2022/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml3-2022/</guid>
      <description>Abstract A determinantal point process (DPP) is an elegant model that assigns a probability to every subset of a collection of items. While conventionally a DPP is parameterized by a symmetric kernel matrix, removing this symmetry constraint, resulting in nonsymmetric DPPs (NDPPs), leads to significant improvements in modeling power and predictive performance. Recent work has studied an approximate Markov chain Monte Carlo (MCMC) sampling algorithm for NDPPs restricted to size- subsets (called -NDPPs).</description>
    </item>
    
    <item>
      <title>Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport</title>
      <link>/publications/2022/miccai-2022/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/miccai-2022/</guid>
      <description>Abstract Connectomics is a popular approach for understanding the brain with neuroimaging data. Yet, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from another atlas. These differences hinder interpreting, generalizing, and combining connectomes and downstream results from different atlases. Recently, it was proposed that a mapping between atlases can be estimated such that connectomes from one atlas (\textit{i.e.}, source atlas) can be reconstructed into a connectome from a different atlas (\textit{i.</description>
    </item>
    
    <item>
      <title>Learning Distributionally Robust Models at Scale via Composite Optimization</title>
      <link>/publications/2022/iclar-2022a/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022a/</guid>
      <description>Abstract To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples– which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods.</description>
    </item>
    
    <item>
      <title>Scalable Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/iclar-2022b/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022b/</guid>
      <description>Abstract A determinantal point process (DPP) on a collection of M items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items. Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well.</description>
    </item>
    
    <item>
      <title>Federated Functional Gradient Boosting</title>
      <link>/publications/2022/karbasi-2022a/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022a/</guid>
      <description>Abstract In this paper, we initiate a study of functional minimization in Federated Learning. First, in the semi-heterogeneous setting, when the marginal distributions of the feature vectors on client machines are identical, we develop the federated functional gradient boosting (FFGB) method that provably converges to the global minimum. Subsequently, we extend our results to the fully-heterogeneous setting (where marginal distributions of feature vectors may differ) by designing an efficient variant of FFGB called FFGB.</description>
    </item>
    
    <item>
      <title>An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks</title>
      <link>/publications/2021/neurips3/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips3/</guid>
      <description>Abstract It is well known that modern deep neural networks are powerful enough to memorize datasets even when the labels have been randomized. Recently, Vershynin (2020) settled a long standing question by Baum (1988), proving that \emph{deep threshold} networks can memorize n points in d dimensions using ˜(e1/δ2+n‾√) neurons and ˜(e1/δ2(d+n‾√)+n) weights, where δ is the minimum distance between the points. In this work, we improve the dependence on δ from exponential to almost linear, proving that ˜(1δ+n‾√) neurons and ˜(dδ+n) weights are sufficient.</description>
    </item>
    
    <item>
      <title>Multiple Descent: Design Your Own Generalization Curve</title>
      <link>/publications/2021/neurips2/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips2/</guid>
      <description>Abstract This paper explores the generalization loss of linear regression in variably parameter- ized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.</description>
    </item>
    
    <item>
      <title>Parallelizing Thompson Sampling</title>
      <link>/publications/2021/neurips4/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips4/</guid>
      <description>Abstract How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon T , our batch Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T ) batch queries.</description>
    </item>
    
    <item>
      <title>Submodular &#43; Concave</title>
      <link>/publications/2021/neurips1/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips1/</guid>
      <description>Abstract It has been well established that first order optimization methods can converge to the maximal objective value of concave functions and provide constant factor approximation guarantees for (non-convex/non-concave) continuous submodular functions. In this work, we initiate the study of the maximization of functions of the form F (x) = G(x) + C (x) over a solvable convex body P, where G is a smooth DR-submodular function and C is a smooth concave function.</description>
    </item>
    
    <item>
      <title>Regularized Submodular Maximization at Scale</title>
      <link>/publications/2021/karbasi-2020j/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020j/</guid>
      <description>Abstract In this paper, we propose scalable methods for maximizing a regularized submodular function f(·) = g(·) − l(·) expressed as the difference between a monotone submodular function g and a modular function l. Indeed, submodularity is inherently related to the notions of diversity, coverage, and representativeness. In particular, finding the mode (i.e., the most likely configuration) of many popular probabilistic models of diversity, such as determinantal point processes, submodular probabilistic models, and strongly log-concave distributions, involves maximization of (regularized) submodular functions.</description>
    </item>
    
    <item>
      <title>Adaptivity in Adaptive Submodularity</title>
      <link>/publications/2021/esfandiari-2019b/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2019b/</guid>
      <description>Abstract Adaptive sequential decision making is one of the central challenges in machine learning and artificial intelligence. In such problems, the goal is to design an interactive policy that plans for an action to take, from a finite set of n actions, given some partial observations. It has been shown that in many applications such as active learning, robotics, sequential experimental design, and active detection, the utility function satisfies adaptive submodularity, a notion that generalizes the notion of diminishing returns to policies.</description>
    </item>
    
    <item>
      <title>Learning and Certification under Instance-targeted Poisoning</title>
      <link>/publications/2021/gao-21/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gao-21/</guid>
      <description>Abstract In this paper, we study PAC learnability and certification under instance-targeted poisoning attacks, where the adversary may change a fraction of the training set with the goal of fooling the learner at a specific target instance. Our first contribution is to formalize the problem in various settings, and explicitly discussing subtle aspects such as learner&amp;rsquo;s randomness and whether (or not) adversary&amp;rsquo;s attack can depend on it. We show that when the budget of the adversary scales sublinearly with the sample complexity, PAC learnability and certification are achievable.</description>
    </item>
    
    <item>
      <title>The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</title>
      <link>/publications/2021/karbasi-2020g/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020g/</guid>
      <description>Abstract Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems.</description>
    </item>
    
    <item>
      <title>Meta Learning in the Continuous Time Limit</title>
      <link>/publications/2021/karbasi-2020c/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020c/</guid>
      <description>Abstract In this paper, we establish the ordinary differential equation (ode) that underlies the train- ing dynamics of Model-Agnostic Meta-Learning (maml). Our continuous-time limit view of the process eliminates the influence of the manually chosen step size of gradient descent and includes the existing gradient descent training algorithm as a special case that results from a specific discretization. We show that the maml ode enjoys a linear convergence rate to an approximate stationary point of the maml loss function for strongly convex task losses, even when the corresponding maml loss is non-convex.</description>
    </item>
    
    <item>
      <title>Regret Bounds for Batched Bandits</title>
      <link>/publications/2021/esfandiari-2021/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2021/</guid>
      <description>Abstract We present simple and efficient algorithms for the batched stochastic multi-armed bandit and batched stochastic linear bandit problems. We prove bounds for their expected regrets that improve over the best known regret bounds for any number of batches. In particular, our algorithms in both settings achieve the optimal expected regrets by using only a logarithmic number of batches. We also study the batched adversarial multi-armed bandit problem for the first time and find the optimal regret, up to logarithmic factors, of any algorithm with predetermined batch sizes.</description>
    </item>
    
    <item>
      <title>Continuous Submodular Maximization: Beyond DR-Submodularity</title>
      <link>/publications/2020/karbasi-2020b/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020b/</guid>
      <description>Abstract n this paper, we propose the first continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called Coordinate-Ascent+, achieves $\frac{e-1}{2e-1}-\epsilon$ a -approximation guarantee while performing $O(n/\epsilon)$ iterations, where the computational complexity of each iteration is roughly $O(n/\sqrt(\epsilon)+n\log n)$(here $n$, denotes the dimension of the optimization problem).</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2020/lin-2019d/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Θ(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Online MAP Inference of Determinantal Point Processes</title>
      <link>/publications/2020/bhaskara-2020/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/bhaskara-2020/</guid>
      <description>Abstract In this paper, we provide an efficient approximation algorithm for finding the most likelihood configuration (MAP) of size k for Determinantal Point Processes (DPP) in the online setting where the data points arrive in an arbitrary order and the algorithm cannot discard the selected elements from its local memory. Given a tolerance additive error $\eta$, our onlinealgorithm achieves a $k^{O(k)}$ multiplicative approximation guarantee with an additive error $\eta$, using a memory footprint independent of the size of the data stream.</description>
    </item>
    
    <item>
      <title>Submodular Maximization Through Barrier Functions</title>
      <link>/publications/2020/karbasi-2020i/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020i/</guid>
      <description>Abstract In this paper, we introduce a novel technique for constrained submodular maximization, inspired by barrier functions in continuous optimization. This connection not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a k-matchoid and l-knapsack constraint (for l ≤ k), we propose a potential function that can be approximately minimized.</description>
    </item>
    
    <item>
      <title>More data can expand the generalization gap between adversarially robust and standard models</title>
      <link>/publications/2020/karbasi-2020h/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020h/</guid>
      <description>Abstract Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models.</description>
    </item>
    
    <item>
      <title>Streaming Submodular Maximization under a k-Set System Constraint. </title>
      <link>/publications/2020/haba-2020/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/haba-2020/</guid>
      <description>Abstract In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a k-matchoid constraint. Moreover, we propose the first streaming algorithm for monotone submodular maximization subject to k-extendible and k-set system constraints. Together with our proposed reduction, we obtain O(klogk) and O(k2logk) approximation ratio for submodular maximization subject to the above constraints, respectively.</description>
    </item>
    
    <item>
      <title>Black Box Submodular Maximization: Discrete and Continuous Settings</title>
      <link>/publications/2020/lin-2020a/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2020a/</guid>
      <description>Abstract In this paper, we consider the problem of black box continuous submodular maximization where we only have access to the function values and no information about the derivatives is provided. For a monotone and continuous DR-submodular function, and subject to a bounded convex body constraint, we propose Black-box Continuous Greedy, a derivative-free algorithm that provably achieves the tight [(1−1/e)OPT−ϵ] approximation guarantee with $O(d/ϵ^3)$ function evaluations. We then extend our result to the stochastic setting where function values are subject to stochastic zero-mean noise.</description>
    </item>
    
    <item>
      <title>One Sample Stochastic Frank-Wolfe</title>
      <link>/publications/2020/mingrui-2020c/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mingrui-2020c/</guid>
      <description>Abstract One of the beauties of the projected gradient descent method lies in its rather simple mechanism and yet stable behavior with inexact, stochastic gradients, which has led to its wide-spread use in many machine learning applications. However, once we replace the projection operator with a simpler linear program, as is done in the Frank-Wolfe method, both simplicity and stability take a serious hit. The aim of this paper is to bring them back without sacrificing the efficiency.</description>
    </item>
    
    <item>
      <title>Quantized frank-wolfe: Faster optimization, lower communication, and projection free</title>
      <link>/publications/2020/karbasi-2020f/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020f/</guid>
      <description>Abstract How can we efficiently mitigate the overhead of gradient communications in distributed optimization? This problem is at the heart of training scalable machine learning models and has been mainly studied in the unconstrained setting. In this paper, we propose Quantised Frank-Wolfe (QFW), the first projection free and communication-efficient algorithm for solving constrained optimization problems at scale. We consider both convex and non-convex objective functions, expressed as a finite-sum or more generally a stochastic optimization problem, and provide strong theoretical guarantees on the convergence rate of QFW.</description>
    </item>
    
    <item>
      <title>Adaptive Sequence Submodularity</title>
      <link>/publications/2019/marko-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/marko-2019a/</guid>
      <description>Abstract In many machine learning applications, one needs to interactively select a sequence of items (e.g., recommending movies based on a user&amp;rsquo;s feedback) or make sequential decisions in a certain order (e.g., guiding an agent through a series of states). Not only do sequences already pose a dauntingly large search space, but we must also take into account past observations, as well as the uncertainty of future outcomes. Without further structure, finding an optimal sequence is notoriously challenging, if not completely intractable.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback</title>
      <link>/publications/2019/minguri-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/minguri-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1−1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1−1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1−1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Stochastic Continuous Greedy &#43;&#43;: When Upper and Lower Bounds Match</title>
      <link>/publications/2019/karbasi-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/karbasi-2019a/</guid>
      <description>Abstract In this paper, we develop $\scg~(\text{SCG}{++})$, the first efficient variant of a conditional gradient method for maximizing a continuous submodular function subject to a convex constraint. Concretely, for a monotone and continuous DR-submodular function, \SCGPP achieves a tight [(1−1/e)\OPT−ϵ] solution while using $O(1/ϵ^2)$ stochastic gradients and O(1/ϵ) calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal $[(1/2)\OPT−ϵ]$ solution with $O(1/ϵ^2)$ stochastic gradients or the tight $[(1−1/e)\OPT−ϵ]$ solution with suboptimal $O(1/ϵ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications</title>
      <link>/publications/2019/harshaw-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/harshaw-2019a/</guid>
      <description>Abstract It is generally believed that submodular functions–and the more general class of γ -weakly submodular functions–may only be optimized under the non-negativity assumption f(S)≥0 . In this paper, we show that once the function is expressed as the difference f=g−c , where g is monotone, non-negative, and γ -weakly submodular and c is non-negative modular, then strong approximation guarantees may be obtained. We present an algorithm for maximizing g−c under a k -cardinality constraint which produces a random feasible set S such that $𝔼[g(S)−c(S)]\geq (1−e−γ−\epsilon)g(OPT)−c(OPT)$ , whose running time is $O(\frac{n}{\epsilon}log2\frac{1}{\epsilon})$ , independent of k .</description>
    </item>
    
    <item>
      <title>Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity. </title>
      <link>/publications/2019/kazemi-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/kazemi-2019a/</guid>
      <description>Abstract Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular function in the streaming setting with a cardinality constraint k . We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only O(k) elements and achieves the tight $\frac{1}{2}$ -approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal $\frac{1}{4}$ -approximation with Θ(k) memory or the optimal 12 -approximation with O(klogk) memory.</description>
    </item>
    
    <item>
      <title>Projection-Free Bandit Convex Optimization</title>
      <link>/publications/2019/lin-2019a/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019a/</guid>
      <description>Abstract In this paper, we propose the first computationally efficient projection-free algorithm for bandit convex optimization (BCO) with a general convex constraint. We show that our algorithm achieves a sublinear regret of $O(nT^{4/5})$ (where T is the horizon and n is the dimension) for any bounded convex functions with uniformly bounded gradients. We also evaluate the performance of our algorithm against baselines on both synthetic and real data sets for quadratic programming, portfolio selection and matrix completion problems.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask</title>
      <link>/publications/2019/ghili-2019/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/ghili-2019/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Unconstrained submodular maximization with constant adaptive complexity</title>
      <link>/publications/2019/lin-2019b/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019b/</guid>
      <description>Abstract In this paper, we consider the unconstrained submodular maximization problem. We propose the first algorithm for this problem that achieves a tight $(1/2−ε)$-approximation guarantee using $O(ε^{−1})$ adaptive rounds and a linear number of function evaluations. No previously known algorithm for this problem achieves an approximation ratio better than 1/3 using less than Ω(n) rounds of adaptivity, where n is the size of the ground set. Moreover, our algorithm easily extends to the maximization of a non-negative continuous DR-submodular function subject to a box constraint, and achieves a tight (1/2−ε)-approximation guarantee for this problem while keeping the same adaptive and query complexities.</description>
    </item>
    
    <item>
      <title>Do Less, Get More: Streaming Submodular Maximization with Subsampling</title>
      <link>/publications/2018/feldman-18a/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/feldman-18a/</guid>
      <description>Abstract In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint).</description>
    </item>
    
    <item>
      <title>Data Summarization at Scale: A Two-Stage Submodular Approach</title>
      <link>/publications/2018/marko-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18b/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summariza- tion tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training func- tions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings.</title>
      <link>/publications/2018/mokhtari-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18b/</guid>
      <description>Abstract In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1−1/e) approximation factor of the optimum global solution using only local computation and communication.</description>
    </item>
    
    <item>
      <title>Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity.</title>
      <link>/publications/2018/lin-18c/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18c/</guid>
      <description>Abstract Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized.</description>
    </item>
    
    <item>
      <title>Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints.</title>
      <link>/publications/2018/kazemi-2018b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018b/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?</title>
      <link>/publications/2018/lin-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18b/</guid>
      <description>Abstract Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for functions obeying properties that generalize submodularity, has been the focus of several recent works.</description>
    </item>
    
    <item>
      <title>Comparison Based Learning from Weak Oracles</title>
      <link>/publications/2018/kazemi-2018a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018a/</guid>
      <description>Abstract There is increasing interest in learning algorithms that involve interaction between hu- man and machine. Comparison-based queries are among the most natural ways to get feed- back from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query.</description>
    </item>
    
    <item>
      <title>Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap</title>
      <link>/publications/2018/mokhtari-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18a/</guid>
      <description>Abstract In this paper, we study the problem of constrained and stochastic continuous submodular maximization. Even though the objective function is not concave (nor convex) and is defined in terms of an expectation, we develop a variant of the conditional gradient method, called Stochastic Continuous Greedy, which achieves a tight approximation guarantee. More precisely, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that Stochastic Continuous Greedy achieves a $[(1−1/e)OPT−\eps]$ guarantee (in expectation) with $O(1/\eps^3)$ stochastic gradient computations.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization</title>
      <link>/publications/2018/lin-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18a/</guid>
      <description>Abstract In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of $O(\sqrt(T))$ (where T is the horizon of the online optimization problem) against a (1−1/e) -approximation to the best feasible solution in hindsight.</description>
    </item>
    
    <item>
      <title>Submodularity on Hypergraphs: From Sets to Sequences</title>
      <link>/publications/2018/marko-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18a/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Gradient Methods for Submodular Maximization</title>
      <link>/publications/2017/hassani-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/hassani-17a/</guid>
      <description>Abstract In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima.</description>
    </item>
    
    <item>
      <title>Interactive Submodular Bandit.</title>
      <link>/publications/2017/lin-17b/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17b/</guid>
      <description>Abstract In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible.</description>
    </item>
    
    <item>
      <title>Streaming Weak Submodularity: Interpreting Neural Networks on the Fly.</title>
      <link>/publications/2017/elenberg-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/elenberg-17a/</guid>
      <description>Abstract In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function.</description>
    </item>
    
    <item>
      <title>Differentially Private Submodular Maximization: Data Summarization in Disguise.</title>
      <link>/publications/2017/marko-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/marko-17a/</guid>
      <description>Abstract Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/baharan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/baharan-17a/</guid>
      <description>Abstract How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution.</description>
    </item>
    
    <item>
      <title>Probabilistic Submodular Maximization in Sub-Linear Time.</title>
      <link>/publications/2017/stan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/stan-17a/</guid>
      <description>Abstract In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting.</title>
      <link>/publications/2017/lin-17a/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17a/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/moran-17a/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/moran-17a/</guid>
      <description>Abstract It is known that greedy methods perform well for maximizing \textitmonotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show—arguably, surprisingly—that invoking the classical greedy algorithm $O(\sqrt(k))$ -times leads to the (currently) fastest deterministic algorithm, called RepeatedGreedy, for maximizing a general submodular function subject to k -independent system constraints. RepeatedGreedy achieves (1+O(1/$\sqrt(k)$))k approximation using $O(nr\sqrt{k})$ function evaluations (here, n and r denote the size of the ground set and the maximum size of a feasible solution, respectively).</description>
    </item>
    
    <item>
      <title>A Submodular Approach to Create Individualized Parcellations of the Human Brain</title>
      <link>/publications/2017/mehraveh-17a/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/mehraveh-17a/</guid>
      <description>Abstract Recent studies on functional neuroimaging (e.g. fMRI) attempt to model the brain as a network. A conventional functional connectivity approach for defining nodes in the network is grouping similar voxels together, a method known as functional parcellation. The majority of previous work on human brain parcellation employs a group-level analysis by collapsing data from the entire population. However, these methods ignore the large amount of inter-individual variability and uniqueness in connectivity.</description>
    </item>
    
    <item>
      <title>Estimating the Size of a Large Network and its Communities from a Random Sample</title>
      <link>/publications/2016/lin-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16c/</guid>
      <description>Abstract Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/lin-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling.</title>
      <link>/publications/2016/lin-16a/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16a/</guid>
      <description>Abstract Learning about the social structure of hidden and hard-to-reach populations — such as drug users and sex workers — is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents.</description>
    </item>
    
    <item>
      <title>Submodular Variational Inference for Network Reconstruction</title>
      <link>/publications/2016/lin-16e/</link>
      <pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16e/</guid>
      <description>Abstract In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions (e.g. phone calls, text messages, social media posts) may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. The process only traverses and thus reveals a limited portion of the edges. The network reconstruction/inference problem is to infer the unrevealed connections.</description>
    </item>
    
    <item>
      <title>Learning network structures from firing patterns.</title>
      <link>/publications/2016/amin-16a/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/amin-16a/</guid>
      <description>Abstract How can we decipher the hidden structure of a network based on limited observations? This question arises in many scenarios ranging from social to wireless and to neural networks. In such settings, we typically observe the nodes&amp;rsquo; behaviors (e.g., the time a node learns about a piece of information, or the time a node gets infected by a disease), and we are interested in inferring the true network over which the diffusion takes place.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Cover: Succinctly Summarizing Massive Data. </title>
      <link>/publications/2015/baharan-15b/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15b/</guid>
      <description>Abstract How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition preva- lent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution.</description>
    </item>
    
    <item>
      <title>Lazier Than Lazy Greedy.</title>
      <link>/publications/2015/baharan-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15a/</guid>
      <description>Abstract Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 − 1/e − ε) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint.</description>
    </item>
    
    <item>
      <title>Submodular Surrogates for Value of Information</title>
      <link>/publications/2015/chen-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15a/</guid>
      <description>Abstract How should we gather information to make effective decisions? A classical answer to this fundamental problem is given by the decision-theoretic value of information. Unfortunately, optimizing this objective is intractable, and myopic (greedy) approximations are known to perform poorly. In this paper, we introduce DiRECt, an efficient yet near-optimal algorithm for nonmyopically optimizing value of information. Crucially, DiRECt uses a novel surrogate objective that is: (1) aligned with the value of information problem (2) efficient to evaluate and (3) adaptive submodular.</description>
    </item>
    
    <item>
      <title>Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning.</title>
      <link>/publications/2015/lucic-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/lucic-15a/</guid>
      <description>Abstract Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry.</description>
    </item>
    
    <item>
      <title>Fast Mixing for Discrete Point Processes</title>
      <link>/publications/2015/rebeschini-15a/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/rebeschini-15a/</guid>
      <description>Abstract We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions μ(S)∝\exp(βf(S)) over all subsets $S\in 2^V$ of a finite set V through a bounded set function $f:2^V→\mathbb{R}$ and a parameter β&amp;gt;0. A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage.</description>
    </item>
    
    <item>
      <title>Sequential Information Maximization: When is Greedy Near-optimal?</title>
      <link>/publications/2015/chen-15b/</link>
      <pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15b/</guid>
      <description>Abstract Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantifies the usefulness of observations is Shannon’s mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations.</description>
    </item>
    
    <item>
      <title>Normalization Phenomena in Asynchronous Networks.</title>
      <link>/publications/2015/karbasi-15b/</link>
      <pubDate>Sun, 03 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15b/</guid>
      <description>Abstract In this work we study a diffusion process in a network that consists of two types of vertices: inhibitory vertices (those obstructing the diffusion) and excitatory vertices (those facilitating the diffusion). We consider a continuous time model in which every edge of the network draws its transmission time randomly. For such an asynchronous diffusion process it has been recently proven that in Erdős-Rényi random graphs a normalization phenomenon arises: whenever the diffusion starts from a large enough (but still tiny) set of active vertices, it only percolates to a certain level that depends only on the activation threshold and the ratio of inhibitory to excitatory vertices.</description>
    </item>
    
    <item>
      <title>Asynchronous decoding of LDPC codes over BEC.</title>
      <link>/publications/2015/haghighatshoar-15a/</link>
      <pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/haghighatshoar-15a/</guid>
      <description>Abstract LDPC codes are typically decoded by running a synchronous message passing algorithm over the corresponding bipartite factor graph (made of variable and check nodes). More specifically, each synchronous round consists of 1) updating all variable nodes based on the information received from the check nodes in the previous round, and then 2) updating all the check nodes based on the information sent from variable nodes in the current round. However, in many applications, ranging from message passing in neural networks to hardware implementation of LDPC codes, assuming that all messages are sent and received at the same time is far from realistic.</description>
    </item>
    
    <item>
      <title>Non-Monotone Adaptive Submodular Maximization.</title>
      <link>/publications/2015/gotovos-15a/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/gotovos-15a/</guid>
      <description>Abstract A wide range of AI problems, such as sensor placement, active learning, and network influence maximization, require sequentially selecting elements from a large set with the goal of optimizing the utility of the selected subset. Moreover, each element that is picked may provide stochastic feedback, which can be used to make smarter decisions about future selections. Finding efficient policies for this general class of adaptive optimization problems can be extremely hard.</description>
    </item>
    
    <item>
      <title>Near-Optimally Teaching the Crowd to Classify.</title>
      <link>/publications/2014/singla-14a/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/singla-14a/</guid>
      <description>Abstract How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners.</description>
    </item>
    
    <item>
      <title>Near Optimal Bayesian Active Learning for Decision Making</title>
      <link>/publications/2014/javdani-14a/</link>
      <pubDate>Wed, 03 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/javdani-14a/</guid>
      <description>Abstract How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations.</description>
    </item>
    
    <item>
      <title>Streaming submodular maximization: massive data summarization on the fly.</title>
      <link>/publications/2014/badandidiyuru-14a/</link>
      <pubDate>Sun, 03 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/badandidiyuru-14a/</guid>
      <description>Abstract How can one summarize a massive data set &amp;ldquo;on the fly&amp;rdquo;, i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of &amp;ldquo;representativeness&amp;rdquo; satisfy submodularity, an intuitive notion of diminishing returns.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization: Identifying Representative Elements in Massive Data.</title>
      <link>/publications/2013/baharan-13a/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/baharan-13a/</guid>
      <description>Abstract Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable, representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Noise-Enhanced Associative Memories</title>
      <link>/publications/2013/karbasi-13d/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13d/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small.</description>
    </item>
    
    <item>
      <title>Iterative Learning and Denoising in Convolutional Neural Associative Memories.</title>
      <link>/publications/2013/karbasi-13b/</link>
      <pubDate>Thu, 03 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once.</description>
    </item>
    
    <item>
      <title>Constrained Binary Identification Problem</title>
      <link>/publications/2013/karbasi-13e/</link>
      <pubDate>Mon, 03 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13e/</guid>
      <description>Abstract We consider the problem of building a binary decision tree, to locate an object within a set by way of the least number of membership queries. This problem is equivalent to the &amp;ldquo;20 questions game&amp;rdquo; of information theory and is closely related to lossless source compression. If any query is admissible, Huffman coding is optimal with close to H[P] questions on average, the entropy of the prior distribution P over objects.</description>
    </item>
    
    <item>
      <title>Coupled neural associative memories.</title>
      <link>/publications/2013/karbasi-13c/</link>
      <pubDate>Fri, 03 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13c/</guid>
      <description>Abstract We propose a novel architecture to design a neural associative memory that is capable of learning a large number of patterns and recalling them later in presence of noise. It is based on dividing the neurons into local clusters and parallel plains, an architecture that is similar to the visual cortex of macaque brain. The common features of our proposed model with those of spatially-coupled codes enable us to show that the performance of such networks in eliminating noise is drastically better than the previous approaches while maintaining the ability of learning an exponentially large number of patterns.</description>
    </item>
    
    <item>
      <title>Comparison-Based Learning with Rank Nets</title>
      <link>/publications/2012/karbasi-12a/</link>
      <pubDate>Wed, 03 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12a/</guid>
      <description>Abstract We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adap- tive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the tar- get in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum.</description>
    </item>
    
    <item>
      <title>Multi-level error-resilient neural networks</title>
      <link>/publications/2012/salavati-12a/</link>
      <pubDate>Thu, 03 May 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/salavati-12a/</guid>
      <description>Abstract The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third. Our work takes a step forward in closing this gap.</description>
    </item>
    
    <item>
      <title>Sequential group testing with graph constraints</title>
      <link>/publications/2012/karbasi-12c/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12c/</guid>
      <description>Abstract In conventional group testing, the goal is to detect a small subset of defecting items D in a large population N by grouping arbitrary subset of N into different pools. The result of each group test T is a binary output depending on whether the group contains a defective item or not. The main challenge is to minimize the number of pools required to identify the set D. Motivated by applications in network monitoring and infection propagation, we consider the problem of group testing with graph constraints.</description>
    </item>
    
    <item>
      <title>Hot or not: Interactive content search using comparisons</title>
      <link>/publications/2012/karbasi-12b/</link>
      <pubDate>Sat, 03 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12b/</guid>
      <description>Abstract In interactive content search through comparisons, a user searching for a target object in a database is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selections. This process is repeated until the target is included in the list presented, at which point the search terminates. We study this problem under the scenario of heterogeneous demand, where target objects are selected from a non-uniform probability distribution.</description>
    </item>
    
    <item>
      <title>Calibration in circular ultrasound tomography devices.</title>
      <link>/publications/2011/parhizkar-11a/</link>
      <pubDate>Sun, 03 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/parhizkar-11a/</guid>
      <description>Abstract We consider the position calibration problem in circular tomography devices, where sensors deviate from a perfect circle. We introduce a new method of calibration based on the time-of-fiight measurements between sensors when the enclosed medium is homogeneous. Bounds on the reconstruction errors are proven and results of simulations mimicking a scanning device are presented.</description>
    </item>
    
    <item>
      <title>Content Search through Comparisons.</title>
      <link>/publications/2011/karbasi-11a/</link>
      <pubDate>Thu, 03 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11a/</guid>
      <description>Abstract We study the problem of navigating through a database of similar objects using comparisons under heterogeneous demand, a problem closely related to small-world network design. We show that, under heterogeneous demand, the small-world network design problem is NP-hard. Given the above negative result, we propose a novel mechanism for small-world network design and provide an upper bound on its performance under heterogeneous demand. The above mechanism has a natural equivalent in the context of content search through comparisons, again under heterogeneous demand; we use this to establish both upper and lower bounds on content search through comparisons.</description>
    </item>
    
    <item>
      <title>Compression with graphical constraints: An interactive browser</title>
      <link>/publications/2011/karbasi-11b/</link>
      <pubDate>Thu, 03 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11b/</guid>
      <description>Abstract We study the problem of searching for a given element in a set of objects using a membership oracle. The membership oracle, given a subset of objects A, and a target object t, determines whether A contains t or not. The goal is to find the target object with the minimum number of questions asked from the oracle. This problem is known to be strongly related to lossless source compression.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2010/cheraghchi-10a/</link>
      <pubDate>Mon, 03 May 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/cheraghchi-10a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>From centralized to distributed sensor localization</title>
      <link>/publications/2010/karbasi-10a/</link>
      <pubDate>Sat, 03 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10a/</guid>
      <description>Abstract In this work we consider the problem of sensor network localization when only the connectivity information is available. More specifically, we compare the performance of the centralized algorithm MDS-MAP with its distributed version HOP-TERRAIN. We show that both algorithms are able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r.</description>
    </item>
    
    <item>
      <title>Distributed sensor network localization from local connectivity: performance analysis for the HOP-TERRAIN algorithm</title>
      <link>/publications/2010/karbasi-10b/</link>
      <pubDate>Wed, 03 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10b/</guid>
      <description>Abstract This paper addresses the problem of determining the node locations in ad-hoc sensor networks when only connectivity information is available. In previous work, we showed that the localization algorithm MDS-MAP proposed by Y. Shang et al. is able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r. The main limitation of MDS-MAP is the assumption that the available connectivity information is processed in a centralized way.</description>
    </item>
    
    <item>
      <title>Calibration for Ultrasound Breast Tomography Using Matrix Completion</title>
      <link>/publications/2010/parhizkar-11a/</link>
      <pubDate>Wed, 03 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/parhizkar-11a/</guid>
      <description>Abstract We study the calibration process in circular ultrasound tomography devices where the sensor positions deviate from the circumference of a perfect circle. This problem arises in a variety of applications in signal processing ranging from breast imaging to sensor network localization. We introduce a novel method of calibration/localization based on the time-of-flight (ToF) measurements between sensors when the enclosed medium is homogeneous. In the presence of all the pairwise ToFs, one can easily estimate the sensor positions using multi-dimensional scaling (MDS) method.</description>
    </item>
    
    <item>
      <title>Support recovery in compressed sensing: An estimation theoretic approach</title>
      <link>/publications/2009/karbasi-09a/</link>
      <pubDate>Tue, 03 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/karbasi-09a/</guid>
      <description>Abstract Compressed sensing (CS) deals with the reconstruction of sparse signals from a small number of linear measurements. One of the main challenges in CS is to find the support of a sparse signal from a set of noisy observations. In the CS literature, several information-theoretic bounds on the scaling law of the required number of measurements for exact support recovery have been derived, where the focus is mainly on random measurement matrices.</description>
    </item>
    
    <item>
      <title>Compressed Sensing with Probabilistic Measurements: A Group Testing Solution</title>
      <link>/publications/2009/cheraghchi-09a/</link>
      <pubDate>Tue, 03 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/cheraghchi-09a/</guid>
      <description>Abstract Detection of defective members of large populations has been widely studied in the statistics community under the name &amp;ldquo;group testing&amp;rdquo;, a problem which dates back to World War II when it was suggested for syphilis screening. There the main interest is to identify a small number of infected people among a large population using collective samples. In viral epidemics, one way to acquire collective samples is by sending agents inside the population.</description>
    </item>
    
    <item>
      <title>A new DOA estimation method using a circular microphone array</title>
      <link>/publications/2007/karbasi-07a/</link>
      <pubDate>Wed, 03 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>/publications/2007/karbasi-07a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method based on circular microphone array. For an arbitrary number of microphones, it is analytically shown that DOA estimation reduces to an efficient non-linear optimization problem. Simulation results demonstrate that deviation of the estimation error for 20 and 10 dB SNR is smaller than 0.7 degree which is comparable to high resolution DOA estimation methods. A larger number of microphones provide a more omni-directional spatial resolution.</description>
    </item>
    
    <item>
      <title>A DOA estimation method for an arbitrary triangular microphone arrangement.</title>
      <link>/publications/2006/karbasi-06a/</link>
      <pubDate>Tue, 03 Jan 2006 00:00:00 +0000</pubDate>
      
      <guid>/publications/2006/karbasi-06a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method for an arbitrary triangular microphone arrangement. Using the phase rotation factors for the crosscorrelations between the adjacent-microphone signals, a general form of the integrated cross spectrum is derived. DOA estimation is reduced to a non-linear optimization problem of the general integrated cross spectrum. It is shown that a conventional DOA estimation for the equilateral triangular microphone arrangement is a special case of the proposed method.</description>
    </item>
    
    <item>
      <title>How Do You Want Your Greedy: Simultaneous or Repeated?</title>
      <link>/publications/2023/jmlr-2023/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/jmlr-2023/</guid>
      <description>Abstract We present SimultaneousGreedys, a deterministic algorithm for constrained submodular maximization. At a high level, the algorithm maintains $l$ solutions and greedily updates them in a simultaneous fashion. SimultaneousGreedys achieves the tightest known approximation guarantees for both $k$-extendible systems and the more general $k$-systems, which are $(k+1)^2/k=k+O(1)$ and $(1+\sqrt{k+2})^2=k+O(\sqrt{k})$, respectively. We also improve the analysis of RepeatedGreedy, showing that it achieves an approximation $\sqrt{\sqrt{k}+O(k)}$ ratio for $k$-systems when allowed to run for $O(k)$ iterations, an improvement in both the runtime and approximation over previous analyses.</description>
    </item>
    
    <item>
      <title>Cross Atlas Remapping via Optimal Transport (CAROT): Creating connectomes for any atlas when raw data is not available</title>
      <link>/publications/2022/nature-2022/</link>
      <pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/nature-2022/</guid>
      <description>Abstract Whether using large-scale projects&amp;mdash;like the Human Connectome Project (HCP), the Adolescent Brain Cognitive Development (ABCD) study, Healthy Brain Network (HBN), and the UK Biobank&amp;mdash;or pooling together several smaller studies, open-source, publicly available datasets allow for unpresented sample sizes and promote generalization efforts. Overall, releasing preprocessing data can enhance participant privacy, democratize science, and lead to unique scientific discoveries. But releasing preprocessed data also limits the choices available to the end-user.</description>
    </item>
    
    <item>
      <title>The Power of Subsampling in Submodular Maximization</title>
      <link>/publications/2021/chris-21/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/chris-21/</guid>
      <description>Abstract We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a (p+2+o(1))-approximation for maximizing a submodular function subject to a p-extendible system using O(n+nk/p) evaluation and feasibility queries, where k is the size of the largest feasible set.</description>
    </item>
    
    <item>
      <title>Individualized functional networks reconfigure with cognitive state</title>
      <link>/publications/2020/mehraveh-2020a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020a/</guid>
      <description>Abstract There is extensive evidence that functional organization of the human brain varies dynamically as the brain switches between task demands, or cognitive states. This functional organization also varies across subjects, even when engaged in similar tasks. To date, the functional network organization of the brain has been considered static. In this work, we use fMRI data obtained across multiple cognitive states (task-evoked and rest conditions) and across multiple subjects, to measure state- and subject-specific functional network parcellation (the assignment of nodes to networks).</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization</title>
      <link>/publications/2020/mokhtaric-20a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mokhtaric-20a/</guid>
      <description>Abstract This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is costly. Instead, stochastic conditional gradient methods are proposed as an alternative solution relying on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction.</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient&#43;&#43;</title>
      <link>/publications/2020/hassani-2019a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/hassani-2019a/</guid>
      <description>Abstract In this paper, we consider the general non-oblivious stochastic optimization where the underlying stochasticity may change during the optimization procedure and depends on the point at which the function is evaluated. We develop Stochastic Frank-Wolfe++ (SFW++), an efficient variant of the conditional gradient method for minimizing a smooth non-convex function subject to a convex body constraint. We show that SFW++ converges to an ϵ-first order stationary point by using $O(1/ϵ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodularity in Action: From Machine Learning to Signal Processing Applications</title>
      <link>/publications/2020/karbasi-2020d/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020d/</guid>
      <description>Abstract Submodularity is a discrete domain functional prop- erty that can be interpreted as mimicking the role of the well- known convexity/concavity properties in the continuous domain. Submodular functions exhibit strong structure that lead to efficient optimization algorithms with provable near-optimality guarantees. These characteristics, namely, efficiency and provable performance bounds, are of particular interest for signal process- ing (SP) and machine learning (ML) practitioners as a variety of discrete optimization problems are encountered in a wide range of applications.</description>
    </item>
    
    <item>
      <title>There is no single functional atlas even for a single individual: Functional parcel definitions change with task</title>
      <link>/publications/2020/mehraveh-2020b/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020b/</guid>
      <description>Abstract The goal of human brain mapping has long been to delineate the functional subunits in the brain and elucidate the functional role of each of these brain regions. Recent work has focused on whole-brain parcellation of functional Magnetic Resonance Imaging (fMRI) data to identify these subunits and create a functional atlas. Functional connectivity approaches to understand the brain at the network level require such an atlas to assess connections between parcels and extract network properties.</description>
    </item>
    
    <item>
      <title> Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences</title>
      <link>/publications/thesis/marko-2020.md/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/marko-2020.md/</guid>
      <description>Student: Marko Mitrovic
Dissertation Title: Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences
Date: Thursday, March 5, 2020 Time: 4:00 PM Location: Room 335, 3rd floor, 17 Hillhouse Avenue
Advisor: Amin Karbasi
Other committee members:
Dan Spielman Dragomir Radev Yaron Singer (Harvard) Abstract :In a nutshell, submodularity covers the class of all problems that exhibit some form of diminishing returns. From a theoretical perspective, this notion of diminishing returns is extremely useful as the resulting mathematical properties allow for provably efficient optimization.</description>
    </item>
    
    <item>
      <title>Computational Neuroscience</title>
      <link>/research/computational_neuroscience_neurimaging.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/computational_neuroscience_neurimaging.md/</guid>
      <description>The human brain is a complex network, consisting of functionally interconnected regions whose coordinated effort gives rise to different functions. Understanding what these regions are, how they interact, and how this interaction forms a wide range of behavior has long been an essential question for human neuroscience. Neuroimaging techniques have provided a unique opportunity to tackle this question in a data-driven way. Advances in neuroimaging techniques such as functional Magnetic Resonance Imaging (fMRI), have allowed us to approximately measure the neural activity in the brain.</description>
    </item>
    
    <item>
      <title>Non-convex Optimization</title>
      <link>/research/non_convex.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/non_convex.md/</guid>
      <description>Until recently, convex programs were seen as the defining boundary for tractability in continuous optimization. However, many problems of interest arising from machine learning and statistical modeling, such as training deep neural networks and learning latent variable models, are glaringly non-convex. While efficient algorithms are known for a few instances of non-convex problems, it remains a central challenge to discover general conditions under which a non-convex problem admits an efficient solution.</description>
    </item>
    
    <item>
      <title>An exemplar-based approach to individualized parcellation reveals the need for sex specific functional networks.</title>
      <link>/publications/2018/mehraveh-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mehraveh-2018a/</guid>
      <description>Abstract Recent work with functional connectivity data has led to significant progress in understanding the functional organization of the brain. While the majority of the literature has focused on group-level parcellation approaches, there is ample evidence that the brain varies in both structure and function across individuals. In this work, we introduce a parcellation technique that incorporates delineation of functional networks both at the individual- and group-level. The proposed technique deploys the notion of “submodularity” to jointly parcellate the cerebral cortex while establishing an inclusive correspondence between the individualized functional networks.</description>
    </item>
    
    <item>
      <title>Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology.</title>
      <link>/publications/2018/karbasi-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/karbasi-2018a/</guid>
      <description>Abstract The connectivity of a neuronal network has a major effect on its functionality and role. It is generally believed that the complex network structure of the brain provides a physiological basis for information processing. Therefore, identifying the network’s topology has received a lot of attentions in neuroscience and has been the center of many research initiatives such as Human Connectome Project. Nevertheless, direct and invasive approaches that slice and observe the neural tissue have proven to be time consuming, complex and costly.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization</title>
      <link>/publications/2016/baharan-16a/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16a/</guid>
      <description>Abstract Many large-scale machine learning problems–clustering, non-parametric learning, kernel machines, etc.–require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Fast Distributed Submodular Cover: Public-Private Data Summarization.</title>
      <link>/publications/2016/baharan-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16c/</guid>
      <description>Abstract In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/baharan-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>From Small-World Networks to Comparison-Based Search.</title>
      <link>/publications/2015/karbasi-15a/</link>
      <pubDate>Thu, 03 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15a/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Noise Facilitation in Associative Memories of Exponential Capacity</title>
      <link>/publications/2014/karbasi-14a/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14a/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns that satisfy certain subspace constraints. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in brain regions thought to operate associatively, such as hippocampus and olfactory cortex. Here we consider associative memories with boundedly noisy internal computations and analytically characterize performance.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance.</title>
      <link>/publications/2014/karbasi-14b/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons. An ideal network should have the ability to 1) learn a set of patterns as they arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries. The majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity.</description>
    </item>
    
    <item>
      <title>Calibration Using Matrix Completion With Application to Ultrasound Tomography.</title>
      <link>/publications/2013/parhizkar-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/parhizkar-13a/</guid>
      <description>Abstract We study the application of matrix completion in the process of calibrating physical devices. In particular we propose an algorithm together with reconstruction bounds for calibrating circular ultrasound tomography devices. We use the time-of-flight (ToF) measurements between sensor pairs in a homogeneous medium to calibrate the system. The calibration process consists of a low-rank matrix completion algorithm to de-noise and estimate random and structured missing ToFs, and the classic multi-dimensional scaling method to estimate the sensor positions from the ToF measurements.</description>
    </item>
    
    <item>
      <title>Robust Localization From Incomplete Local Information</title>
      <link>/publications/2013/karbasi-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13a/</guid>
      <description>Abstract We consider the problem of localizing wireless devices in an ad hoc network embedded in a d-dimensional Euclidean space. Obtaining a good estimate of where wireless devices are located is crucial in wireless network applications including environment monitoring, geographic routing, and topology control. When the positions of the devices are unknown and only local distance information is given, we need to infer the positions from these local distance measurements. This problem is particularly challenging when we only have access to measurements that have limited accuracy and are incomplete.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2012/cheraghchi-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/cheraghchi-12a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>Low-Rank Matrix Approximation Using Point-Wise Operators</title>
      <link>/publications/2012/amini-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/amini-12a/</guid>
      <description>Abstract The problem of extracting low-dimensional structure from high-dimensional data arises in many applications such as machine learning, statistical pattern recognition, wireless sensor networks, and data compression. If the data is restricted to a lower dimensional subspace, then simple algorithms using linear projections can find the subspace and consequently estimate its dimensionality. However, if the data lies on a low-dimensional but nonlinear space (e.g., manifolds), then its structure may be highly nonlinear and, hence, linear methods are doomed to fail.</description>
    </item>
    
    <item>
      <title>Group Testing With Probabilistic Tests: Theory, Design and Application.</title>
      <link>/publications/2011/cheraghchi-11a/</link>
      <pubDate>Sat, 03 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/cheraghchi-11a/</guid>
      <description>Abstract Identification of defective members of large populations has been widely studied in the statistics community under the name of group testing. It involves grouping subsets of items into different pools and detecting defective members based on the set of test results obtained for each pool. In a classical noiseless group testing setup, it is assumed that the sampling procedure is fully known to the reconstruction algorithm, in the sense that the existence of a defective member in a pool results in the test outcome of that pool to be positive.</description>
    </item>
    
    <item>
      <title>Online Learning</title>
      <link>/research/online_learning.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/online_learning.md/</guid>
      <description>In many practical applications, the environment is so complex that it may be infeasible to lay out a precise model and use the classical mathematical optimization methods. It is then necessary, and very often beneficial, to consider a robust approach, by considering optimization as a process that learns from experience as more aspects of the problem are being observed. This view of optimization as a process has become prominent in various fields and led to many successes in modeling and systems.</description>
    </item>
    
    <item>
      <title>Allocating tasks to machines in computing clusters</title>
      <link>/publications/2014/karbasi-14c/</link>
      <pubDate>Mon, 03 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14c/</guid>
      <description>Abstract Allocating tasks to machines in computing clusters is described. In an embodiment a set of tasks associated with a job are received at a scheduler. In an embodiment an index can be computed for each combination of tasks and processors and stored in a lookup table. In an example the index may be include an indication of the preference for the task to be processed on a particular processor, an indication of a waiting time for the task to be processed and an indication of how other tasks being processed in the computing cluster may be penalized by assigning a task to a particular processor.</description>
    </item>
    
    <item>
      <title>Hot or Not: Interactive Content Search Using Comparisons</title>
      <link>/publications/2013/karbasi-13f/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13f/</guid>
      <description>Abstract In comparison-based active learning, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Submodular Optimization</title>
      <link>/research/submodular_optimization.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/submodular_optimization.md/</guid>
      <description>The difficulty of searching through a massive amount of data in order to quickly make an informed decision is one of today’s most ubiquitous challenges. Many scientific and engineering models feature inherently discrete decision variables—from phrases in a corpus to objects in an image. Similarly, nearly all aspects of the machine learning pipeline involve discrete tasks, from data summarization and sketching to feature selection and model explanation. The study of how to make near-optimal decisions from a massive pool of possibilities is at the heart of combinatorial optimization.</description>
    </item>
    
    <item>
      <title>Javid Dadashkarimi Graduated</title>
      <link>/news/javid-2023.md/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/news/javid-2023.md/</guid>
      <description>Chris Harshaw Graduated with his PhD thesis entitled “Data Driven Mapping between Functional Connectome Using Optimal Trasnport”.
Adviserr: Dustin Scheinost and Amin Karbasi
Abstract:</description>
    </item>
    
    <item>
      <title>Stochastic Processes</title>
      <link>/courses/stochastic23.md/</link>
      <pubDate>Tue, 03 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/courses/stochastic23.md/</guid>
      <description>Course ID: S&amp;amp;DS 351, MATH 251, S&amp;amp;DS 551, ENAS 502 In this course we will go through the following topics:
Review important notions of probability Expectation Convergence of random variables Law of large numbers Poisson Processes Renewal Processes Markov Chains and Random Walks Martingales If time permits, we will look at a few interesting applications of the above topics. Textbook: We mainly use &amp;ldquo;Introduction to Stochastic Processes&amp;rdquo; by Erhan Cinlar</description>
    </item>
    
    <item>
      <title>Qinghao Liang won Best Paper award at Graphs in Biomedical Imaging</title>
      <link>/news/grail-2022.md/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/news/grail-2022.md/</guid>
      <description>Qinghao Liang, Javid Dadashkarimi, Wei Dai, Amin Karbasi, Joseph Chang, Harrison H. Zhou, and Dustin Scheinost won best paper award at 4th Workshop on GRaphs in biomedicAl Image anaLysis (GRAIL) at MICCAI 2022 for the paper &amp;ldquo;Transforming connectomes to “any” parcellation via graph matching&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Jane Received a Graduate Fellowship for STEM diversity from NSA</title>
      <link>/news/jane-2022.md/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/news/jane-2022.md/</guid>
      <description>Jane Lee received Graduate Fellowship for STEM diversity from NSA.</description>
    </item>
    
    <item>
      <title>Amin Karbasi wins Bell Labs Prize for Brain Mapping Technology</title>
      <link>/news/bell-2022.md/</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/bell-2022.md/</guid>
      <description>Prof. Amin Karbasi and Mehraveh Salehi (Ph.D. ’19, Electrical Engineering) won second place at Nokia’s Bell Labs Prize ceremony Tuesday for their work on understanding how information flows in the human brain based on different cognitive tasks. The team says their innovation makes a concrete connection between artificial intelligence and natural intelligence.
The Bell Labs Prize recognizes disruptive innovations that solve the key challenges facing humanity. Eight months ago, numerous academics from across the world applied to work with Nokia researchers to help advance their innovations.</description>
    </item>
    
    <item>
      <title>Chris Harshaw Graduated</title>
      <link>/news/chris-2021.md/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/chris-2021.md/</guid>
      <description>Chris Harshaw Graduated with his PhD thesis entitled “Algorithmic Advances for the Design and Analysis of Randomized Experiments”.
Adviserr: Daniel Spielman and Amin Karbasi
Abstract: Randomized experiments are the gold standard for investigating the causal effect of treatment on a population. In this dissertation, we present algorithmic advances for three different problems arising in the design and analysis of randomized experiments: covariate balancing, variance estimation, and bipartite experiments. In the first chapter, we describe an inherent trade-off between covariate balancing and robustness, which we formulate as a distributional discrepancy problem.</description>
    </item>
    
    <item>
      <title>NSF invests The Institute for Learning-enabled Optimization at Scale</title>
      <link>/news/karbasi-21.md/</link>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-21.md/</guid>
      <description>The TILOS mission is to make impossible optimizations possible, at scale and in practice. Our research will pioneer learning-enabled optimizations that transform chip design, robotics, networks and other use domains that are vital to our nation’s health, prosperity and welfare. TILOS is a National Science Foundation funded National Artificial Intelligence (AI) Research Institute: TILOS
TIIOS is a partnership of faculty from University of California, San Diego, Massachusetts Institute of Technology, National University, University of Pennsylvania, University of Texas at Austin, and Yale University.</description>
    </item>
    
    <item>
      <title>ICML 2021 Workshop on Overparameterization: Pitfalls &amp; Opportunities</title>
      <link>/news/icml-2021.md/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/icml-2021.md/</guid>
      <description>Yasaman Bahri Research Scientist
Google Research, Brain Team
Quanquan Gu Department of Computer Science
University of California, Los Angeles
Amin Karbasi Yale Institute for Network Science Yale University
Hanie Sedghi Research Scientist
Google Brain
Brief Description and Outline Modern machine learning models are often highly overparameterized. The prime examples of late are neural network architectures that can achieve state-of-the-art performance while having many more parameters than the number of training examples.</description>
    </item>
    
    <item>
      <title>Mingrui Zhang Graduated</title>
      <link>/news/mingrui-2021.md/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/news/mingrui-2021.md/</guid>
      <description>Mingrui Zhang Graduated with his PhD thesis entitled “Scalable Projection-Free Optimization”.
Adviserr: Amin Karbasi
Abstract: As a projection-free algorithm, Frank-Wolfe (FW) method, also known as conditional gradient, has recently received considerable attention in the machine learning community. In this dissertation1, we study several topics on the FW variants for scalable projection-free optimization. We first propose 1-SFW, the first projection-free method that requires only one sample per iteration to update the optimization variable and yet achieves the best known complexity bounds for convex, non-convex, and monotone DR-submodular settings.</description>
    </item>
    
    <item>
      <title>Scalable Projection-Free Optimization</title>
      <link>/publications/thesis/mingrui-21.md/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mingrui-21.md/</guid>
      <description>Adviserr: Amin Karbasi
Abstract: As a projection-free algorithm, Frank-Wolfe (FW) method, also known as conditional gradient, has recently received considerable attention in the machine learning community. In this dissertation1, we study several topics on the FW variants for scalable projection-free optimization. We first propose 1-SFW, the first projection-free method that requires only one sample per iteration to update the optimization variable and yet achieves the best known complexity bounds for convex, non-convex, and monotone DR-submodular settings.</description>
    </item>
    
    <item>
      <title>ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</title>
      <link>/icml/icml-20.md/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/icml/icml-20.md/</guid>
      <description>Hamed Hassani School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104
Amin Karbasi Yale Institute for Network Science Yale University New Haven, CT 06520
Slides Module 1 Module 2 Module 3 Module 4 Videos Part I Part II Part III Part IV Brief Description and Outline This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications.</description>
    </item>
    
    <item>
      <title>ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back</title>
      <link>/news/icml-20.md/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/icml-20.md/</guid>
      <description>Hamed Hassani School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104
Amin Karbasi Yale Institute for Network Science Yale University New Haven, CT 06520
Slides Module 1 Module 2 Module 3 Module 4 Videos Part I Part II Part III Part IV Brief Description and Outline This tutorial will cover recent advancements in discrete optimization methods for large-scale machine learning. Traditionally, machine learning has been harnessing convex optimization to design fast algorithms with provable guarantees for a broad range of applications.</description>
    </item>
    
    <item>
      <title>Amin Karbasi is promoted to Associate Professor</title>
      <link>/news/amin-2020.md/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/amin-2020.md/</guid>
      <description>Amin Karbasi is promoted to Associate Professor.</description>
    </item>
    
    <item>
      <title>Hunala is launched!</title>
      <link>/news/hunala-2020.md/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/hunala-2020.md/</guid>
      <description>Yale app Hunala aims to be ‘Waze for coronavirus’ Yale News: A team of Yale researchers has developed a new app, Hunala, that aims to be the “Waze for coronavirus Led by Sterling Professor Nicholas Christakis, a physician and social networks expert, with colleagues in the Yale School of Engineering and Applied Science, the free app provides a daily snapshot of personal and regional risk for COVID-19 infection based on data from the Centers for Disease Control and Prevention and users’ self-reported health and demographic information.</description>
    </item>
    
    <item>
      <title>Marko Mitrovic Graduated</title>
      <link>/news/marko-2020.md/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/marko-2020.md/</guid>
      <description>Student: Marko Mitrovic
Dissertation Title: Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences
Date: Thursday, March 5, 2020 Time: 4:00 PM Location: Room 335, 3rd floor, 17 Hillhouse Avenue
Advisor: Amin Karbasi
Other committee members:
Dan Spielman Dragomir Radev Yaron Singer (Harvard) Abstract :In a nutshell, submodularity covers the class of all problems that exhibit some form of diminishing returns. From a theoretical perspective, this notion of diminishing returns is extremely useful as the resulting mathematical properties allow for provably efficient optimization.</description>
    </item>
    
    <item>
      <title>Lin Chen Graduated</title>
      <link>/news/lin-2020.md/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/lin-2020.md/</guid>
      <description>The final doctoral examination for Lin Chen will take place on Wednesday, March 11th, at 2:00pm in HLH17, Room 335.
The title of the thesis is: Online Optimization: Convex and Submodular Functions
Advisor: Amin Karbasi
Members of the Committee are:
Professor Negahban Professor Spielman Abstract: In the first part, we study switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it.</description>
    </item>
    
    <item>
      <title>Online Optimization: Convex and Submodular Functions</title>
      <link>/publications/thesis/lin-2020.md/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/lin-2020.md/</guid>
      <description>The final doctoral examination for Lin Chen will take place on Wednesday, March 11th, at 2:00pm in HLH17, Room 335.
The title of the thesis is: Online Optimization: Convex and Submodular Functions
Advisor: Amin Karbasi
Members of the Committee are:
Professor Negahban Professor Spielman Abstract: In the first part, we study switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it.</description>
    </item>
    
    <item>
      <title>Brain Initiative Trainee Award 2020</title>
      <link>/news/dadashkarimi-20.md/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/news/dadashkarimi-20.md/</guid>
      <description>Javid Dadashkarimi won Brain Initiative Trainee Award for `A web-based toolkit for visualizing and interpreting complex connectomic results in BISWeb &amp;lsquo;. This award is given to students who had significant research impact that can create a dynamic understanding of brain function.</description>
    </item>
    
    <item>
      <title>Interactive Decision Making</title>
      <link>/research/interactive_decision_making.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/research/interactive_decision_making.md/</guid>
      <description>In computer science, and machine learning in particular, the primary purpose of many systems is to help humans make decisions. Simultaneously, many of these systems also stand to benefit from having a human in the loop, whether it is to reinforce good decisions, warn against bad decisions, or simply to provide expert advice in areas of uncertainty.
A simple, concrete example can be seen in recommender systems. Whether it is through explicit feedback (such as rating a movie on Netflix) or implicit feedback (such as clicking/not clicking on an advertisement), the vast majority of successful, real-world recommender systems are constantly interacting with and adapting to each user.</description>
    </item>
    
    <item>
      <title>Open Positions at INFERENCE, INFORMATION, AND DECISION GROUP</title>
      <link>/openings/openings.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/openings.md/</guid>
      <description>PhD students The I.I.D. Systems Group at the Yale Institute of Network Science has an open position for a Ph.D. student. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Many problems studied in our group have a strong interdisciplinary component. See our research page for an overview of the projects that we are working on.</description>
    </item>
    
    <item>
      <title>Open Positions at INFERENCE, INFORMATION, AND DECISION GROUP</title>
      <link>/openings/openings.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/openings.md/</guid>
      <description>Postdoc The I.I.D. Group at Yale University has a postdoctoral open position. We are looking for an applicant who has a strong background and interest in theoretical machine learning, statistics, and optimization. Many problems studied in our group have interdisciplinary components, including computational neuroscience, healthcare, and social networks. See our research page for an overview of the projects that we are working on. The postdoctoral fellow will have the flexibility to pursue any topics within our ongoing areas of research such as discrete and continuous optimization, online and reinforcement learning, statistical learning theory, etc.</description>
    </item>
    
    <item>
      <title>PhD students</title>
      <link>/openings/phd.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/phd.md/</guid>
      <description>The I.I.D. Systems Group at the Yale Institute of Network Science has an open position for a Ph.D. student. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Many problems studied in our group have a strong interdisciplinary component. See our research page for an overview of the projects that we are working on.</description>
    </item>
    
    <item>
      <title>Postdocs</title>
      <link>/openings/postdoc.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/openings/postdoc.md/</guid>
      <description>The I.I.D. Systems Group at the Yale Institute of Network Science has open positions for a postdoctoral scholar. We are looking for exceptional applicants who have a strong background in areas such as machine learning, statistics, optimization, theoretical computer science and distributed systems. Applicants for the postdoctoral scholar position need to have a strong publication record at the relevant conferences and journals. Many problems studied in our group have a strong interdisciplinary component.</description>
    </item>
    
    <item>
      <title>Stochastic Processes</title>
      <link>/courses/stochastic.md/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/courses/stochastic.md/</guid>
      <description>Course ID: S&amp;amp;DS 351, MATH 251, S&amp;amp;DS 551, ENAS 502 In this course we will go through the following topics:
Review important notions of probability Expectation Convergence of random variables Law of large numbers Poisson Processes Renewal Processes Markov Chains and Random Walks Martingales If time permits, we will look at a few interesting applications of the above topics. Textbook: We mainly use &amp;ldquo;Introduction to Stochastic Processes&amp;rdquo; by Erhan Cinlar</description>
    </item>
    
    <item>
      <title>Individualized and Task-Specific Functional Brain Mapping</title>
      <link>/publications/thesis/mehraveh-19.md/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mehraveh-19.md/</guid>
      <description>Mehraveh Salehi Graduated with her PhD thesis entitled “Individualized and Task-Specific Human Brain Mapping”.
The final doctoral examination for Mehraveh Salehi took place on Tuesday, September 3rd, at 11:00am at YINS, 17 Hillhouse, 3rd floor.
The title of the thesis is: Individualized and Task-Specific Functional Brain Mapping
Adviserr:
Amin Karbasi Todd Constable Members of the Committee are:
Professor Papademetris Professor Tassiulas Professor Jeff Bilmes Abstract: Understanding the human brain, with its remarkable ability to control higher thought, behavior, and memory, remains one of the greatest intellectual challenges in all of science.</description>
    </item>
    
    <item>
      <title>Mehraveh Salehi Graduated</title>
      <link>/news/mehraveh-19.md/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19.md/</guid>
      <description>Mehraveh Salehi Graduated with her PhD thesis entitled “Individualized and Task-Specific Human Brain Mapping”.
The final doctoral examination for Mehraveh Salehi took place on Tuesday, September 3rd, at 11:00am at YINS, 17 Hillhouse, 3rd floor.
The title of the thesis is: Individualized and Task-Specific Functional Brain Mapping
Adviserr:
Amin Karbasi Todd Constable Members of the Committee are:
Professor Papademetris Professor Tassiulas Professor Jeff Bilmes Abstract: Understanding the human brain, with its remarkable ability to control higher thought, behavior, and memory, remains one of the greatest intellectual challenges in all of science.</description>
    </item>
    
    <item>
      <title>NSF CAREER Award 2019</title>
      <link>/news/karbasi-19a.md/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-19a.md/</guid>
      <description>Amin Karbasi won NSF CAREER Award 2019: link
ABSTRACT: The difficulty of searching through a massive amount of data in order to quickly make an informed decision is one of today&amp;rsquo;s most ubiquitous challenges. Many scientific and engineering models feature data with inherently discrete characteristics, where discrete means that the data takes on a finite set of possible values. Examples of such data include phrases in text to objects in an image.</description>
    </item>
    
    <item>
      <title>Facebook-Main Award 2019 for Mehraveh Salehi</title>
      <link>/news/mehraveh-19b.md/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19b.md/</guid>
      <description>Facebook-Main Award for Mehraveh Salehi. Congratulations Mehraveh! She wins FACEBOOK-MAIN AWARD as a leading woman in AI-Neuroscience. Here is her very accessible presentation on the role of submodularity on decoding brain states.</description>
    </item>
    
    <item>
      <title>Ivy 3-Minute Thesis Competition Award</title>
      <link>/news/mehraveh-19c.md/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-19c.md/</guid>
      <description>On April 25, 2019, Columbia University and the United Nations hosted the first Ivy-wide 3-Minute Thesis Competition to celebrate the diverse scholarly work that PhD students are doing across the Ivy League community. Following opening remarks by Elliott Harris, UN Assistant Secretary-General and Chief Economist, fourteen PhD students from Brown University, Columbia University, Cornell University, Dartmouth College, Princeton University, the University of Pennsylvania and Yale gave presentations about their research in just 3 minutes.</description>
    </item>
    
    <item>
      <title>Amazon Research Award</title>
      <link>/news/karbasi-19b.md/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-19b.md/</guid>
      <description>Amin Karbasi won Amazon Research Award! link
The ARA program funds projects conducted primarily by PhD students or post docs, under the supervision of the faculty member awarded the funds. To encourage collaboration and the sharing of insights, each funded proposal team is assigned an appropriate Amazon research contact. We also invite ARA recipients to speak at Amazon offices worldwide about their work and to meet with our research groups face-to-face, and encourage ARA recipients to publish their research outcome and commit related code to open-source code repositories.</description>
    </item>
    
    <item>
      <title>Understanding Ourselves Through Neuroimaging and Algorithms</title>
      <link>/news/mehraveh-18a.md/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-18a.md/</guid>
      <description>Understanding Ourselves Through Neuroimaging and Algorithms: link
Combining neuroscience with algorithms and network science, Yale researchers have developed a method of analyzing the neuronal connections of individual brains that allow them to successfully predict the subjects’ IQs, their sex, and even tasks they were performing at the time of the brain scan.
In a collaboration between the labs of Amin Karbasi, assistant professor of electrical engineering &amp;amp; computer science and Todd Constable, professor of radiology and biomedical imaging and of neurosurgery, the researchers analyzed the functional MRI scans of more than 100 subjects from the Human Connectome Project, a five-year effort to create a network map of the human brain.</description>
    </item>
    
    <item>
      <title>Dynamic and Discrete Optimization</title>
      <link>/courses/fall-19.md/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-19.md/</guid>
      <description>Course ID: EENG 433 The study of fundamental techniques in discrete optimization and its numerous industry applications, including airline scheduling, telecommunication routing, recommender systems, and predicting financial markets. Topics include linear programs, dynamic programs, finite and infinite state scenarios, bandit optimization, and potentially submodular functions and relationships between discrete and continuous optimization methods through the lens of submodularity. Familiarity with discrete mathematics, algorithms, combinatorics, and calculus is assumed.
Meeting Info TTh 1pm-2:15pm in BCT CO31</description>
    </item>
    
    <item>
      <title>ONR Young Investigator Award 2019</title>
      <link>/news/karbasi-18a.md/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-18a.md/</guid>
      <description>Amin Karbasi won ONR Young Investigator Award 2019
Area: Robust and Interactive Information Gathering in Dynamic Environments.</description>
    </item>
    
    <item>
      <title>ISIT 2018 Tutorial on Submodularity</title>
      <link>/news/isit-18a.md/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/isit-18a.md/</guid>
      <description>ISIT 2018 Tutorial on Submodularity in Information and Data Science.
Submodularity is a structural property over functions that has received significant attention in the mathematics community, owing to their natural and wide ranging applicability. In particular, numerous challenging problems in information theory, machine learning, and artificial intelligence rely on optimization techniques for which submodularity is key to solving them efficiently.We will start by defining submodularity and polymatroidality — we will survey a surprisingly diverse set of functions that are submodular and operations that preserve submodularity.</description>
    </item>
    
    <item>
      <title>CoderPortfolioの特徴</title>
      <link>/posts/featuresofcoderportfolio.ja/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/featuresofcoderportfolio.ja/</guid>
      <description>変更点 ShortCode &amp;ldquo;private content&amp;quot;を追加しました 綺麗なサイトを崩さずに、あなたの個人的なコンテンツ（趣味や感情）を簡単に伝えることができます。
実際の動きについては、&amp;ldquo;テーマデモ&amp;quot;をご覧ください。
リンクに赤い下線のアニメーションを追加しました サイトがよりカラフルに美しくなりました。
SNSシェアのボタンを追加しました あなたの記事がより影響力を持つようになりました。
コードのハイライトを変更しました 日本のサービス Qiita のハイライトを参照しました。
とても優しく美しいハイライトです。
ヘッダーに影を追加しました より境界線がはっきりしました。
ShortCode &amp;ldquo;portfolio&amp;quot;を追加しました あなたの作品を綺麗に表示できるようになりました。 実際の動きについては、&amp;ldquo;テーマデモ&amp;quot;をご覧ください。</description>
    </item>
    
    <item>
      <title>Features Of CoderPortfolio</title>
      <link>/posts/featuresofcoderportfolio/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/featuresofcoderportfolio/</guid>
      <description>Change Point &amp;ldquo;private content&amp;rdquo; short code added. It is now possible to communicate your personal hobby and your own content.
You can use it easily. Please see &amp;ldquo;theme-demo&amp;rdquo; for details and demo.
An animation of red underline was added to Anchor. The site became a little fun and colorful.
The button of the SNS share was added. It became to have an influence when writing articles more.
Changed the color scheme of code highlight.</description>
    </item>
    
    <item>
      <title>Theme Demo</title>
      <link>/posts/theme-demo/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/theme-demo/</guid>
      <description>Style Demo h1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading This is bold text
This is bold text
This is italic text
This is italic text
Deleted text
This is text with inline math $\sum_{n=1}^{\infty} 2^{-n} = 1$ and with math blocks:
$$ \sum_{n=1}^{\infty} 2^{-n} = 1 $$
Heading Another heading text text text text text text Block quotes are written like so.
They can span multiple paragraphs, if you like.</description>
    </item>
    
    <item>
      <title>テーマデモ</title>
      <link>/posts/theme-demo.ja/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/theme-demo.ja/</guid>
      <description>Style Demo h1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading This is bold text
This is bold text
This is italic text
This is italic text
Deleted text
This is text with inline math $\sum_{n=1}^{\infty} 2^{-n} = 1$ and with math blocks:
$$ \sum_{n=1}^{\infty} 2^{-n} = 1 $$
Heading Another heading text text text text text text Block quotes are written like so.
They can span multiple paragraphs, if you like.</description>
    </item>
    
    <item>
      <title>CVPR 2018 Tutorial on Big Data Summarization</title>
      <link>/news/cvp4-18a.md/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/cvp4-18a.md/</guid>
      <description>CVPR 2018 tutorial on Big Data Summarization: Algorithms and Applications.
The increasing amounts of data in computer vision requires robust tools to extract most important information from large collections of data. The summarization problem addresses this challenge by finding a small subset of most informative data points from large datasets. However, summarization often leads to optimization programs that are nonconvex and NP-hard. While (non)convex programming and submodular optimization have been studied intensively in mathematics, successful and effective applications of them for the problem of information summarization along with new theoretical results have recently emerged.</description>
    </item>
    
    <item>
      <title>Google PhD felloswhip</title>
      <link>/news/lin-18a.md/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/lin-18a.md/</guid>
      <description>Lin Chen received Google PhD felloswhip 2018. Congrats Lin.</description>
    </item>
    
    <item>
      <title>Dynamic and Discrete Optimization</title>
      <link>/courses/spring-18.md/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-18.md/</guid>
      <description>Course ID: EENG 400 The study of fundamental techniques in discrete optimization and its numerous industry applications, including airline scheduling, telecommunication routing, recommender systems, and predicting financial markets. Topics include linear programs, dynamic programs, finite and infinite state scenarios, bandit optimization, and potentially submodular functions and relationships between discrete and continuous optimization methods through the lens of submodularity. Familiarity with discrete mathematics, algorithms, combinatorics, and calculus is assumed.
Meeting Info TTh 1pm-2:15pm in HLH17 03</description>
    </item>
    
    <item>
      <title>AFOSR Young Investigator Research Award 2018</title>
      <link>/news/karbasi-17a.md/</link>
      <pubDate>Wed, 11 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/karbasi-17a.md/</guid>
      <description>Amin Karbasi won AFOSR Young Investigator Research Award 2018!
ARLINGTON, Virginia &amp;ndash; The Air Force Office of Scientific Research today announced it will award approximately $19.9 million in grants to 45 scientists and engineers from 38 research institutions and small businesses who submitted winning research proposals through the Air Force&amp;rsquo;s Young Investigator Research Program (YIP).
The YIP is open to scientists and engineers at research institutions across the United States who received Ph.</description>
    </item>
    
    <item>
      <title>NIPS 2017 workshop</title>
      <link>/news/nips-17.md/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/nips-17.md/</guid>
      <description>We are organizing a workshop at NIPS 2017 on Discrete Structures in Machine Learning.
Abstract: Traditionally, machine learning has been focused on methods where objects reside in continuous domains. The goal of this workshop is to advance state-of-the-art methods in machine learning that involve discrete structures.
Models with ultimately discrete solutions play an important role in machine learning. At its core, statistical machine learning is concerned with making inferences from data, and when the underlying variables of the data are discrete, both the tasks of model inference as well as predictions using the inferred model are inherently discrete algorithmic problems.</description>
    </item>
    
    <item>
      <title>Simons Research Fellowship 2017</title>
      <link>/news/simons-16.md/</link>
      <pubDate>Sat, 02 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/simons-16.md/</guid>
      <description>Amin Karbasi received Simons Research Fellowship 2017 for the “Foundations of Machine Learning” program: link
T​he goal of this program is to grow the reach and impact of CS theory within machine learning.
One central component of the program will be ​formalizing basic questions in developing areas of practice​, and gaining fundamental insights into these. Target areas of particular interest are ​interactive learning​ and representation learning. Interactive learning consists of scenarios in which the communication between human and learner is richer than a one-­way transmission of labeled examples; this happens, for instance, in teaching, or explanation-based learning, and in crowdsourcing.</description>
    </item>
    
    <item>
      <title>Microsoft Azure Research Award 2017</title>
      <link>/news/azure-17.md/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/azure-17.md/</guid>
      <description>Amin Karbasi Received Microsoft Azure Research Award 2017</description>
    </item>
    
    <item>
      <title>MICCAI Young Scientist Award</title>
      <link>/news/mehraveh-17a.md/</link>
      <pubDate>Mon, 09 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/news/mehraveh-17a.md/</guid>
      <description>MICCAI Young Scientist Award for ”A Submodular Approach to Create Individualized Parcellations of Human Brain”</description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-17.md/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-17.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>Grainger Award 2017</title>
      <link>/news/frontier-16.md/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/news/frontier-16.md/</guid>
      <description>Grainger Award 2017 from National Academy of Engineering for Advancement of Interdisciplinary Research.
Amin Karbasi (Yale University) and Amit Surana (United Technologies Research Center) have received a Grainger Grant to “develop a unified approach for saliency detection in heterogeneous temporal data.” The grant will support the team’s interdisciplinary research to develop algorithms for compressing massive amounts of time—varying data into small salient or informative datasets to allow faster decision making. For example, if salient or significant images in particular frames can be identified from terabytes of video stream, the original video can be summarized using a much smaller set of frames, enabling much faster video processing for surveillance applications such as anomaly detection and activity classification.</description>
    </item>
    
    <item>
      <title>Stochastic Processes</title>
      <link>/courses/spring-16.md/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-16.md/</guid>
      <description>Course ID: ENAS 496, ENAS 502, STAT 251, STAT 551, MATH 251 Introduction to the study of random processes including linear prediction and Kalman filtering, Poison counting process and renewal processes, Markov chains, branching processes, birth-death processes, Markov random fields, martingales, and random walks. Applications chosen from communications, networking, image reconstruction, Bayesian statistics, finance, probabilistic analysis of algorithms, and genetics and evolution.
Meeting Info MW 1pm-2:15pm in WLH 208
Final Exam Tuesday, May 10, 2016 at 7pm</description>
    </item>
    
    <item>
      <title>AISTATS Best Student Paper Award</title>
      <link>/news/aistats-15.md/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/news/aistats-15.md/</guid>
      <description>Our paper Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning received the Best Student Paper Award from the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.</description>
    </item>
    
    <item>
      <title>Google Faculty Research Award</title>
      <link>/news/google-16.md/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/news/google-16.md/</guid>
      <description>Amin Karbasi received Google Faculty Research Award 2015:link
Awardees:
Adnan Darwiche, University of California, Los Angeles Alexandr Andoni, Columbia University Amin Karbasi, Yale University Amir Ali Ahmadi, Princeton University Animashree Anandkumar, University of California, Irvine Barna Saha, University of Massachusetts - Amherst Emmanuel Abbe, Princeton University Fei Sha, University of California, Los Angeles J. Zico Kolter, Carnegie Mellon University Lei Xing, Stanford University Li Ma, Duke University Marc Deisenroth, Imperial College London Mehryar Mohri, New York University Michela Milano, University of Bologna Raquel Urtasun, University of Toronto Stefanie Jegelka, Massachusetts Institute of Technology Anirban Dasgupta - Indian Institute of Technology Gandhinagar Ronojoy Adhikari - Institute of Mathematical Sciences Chennai </description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-15.md/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-15.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>Probability and Stochastic Procsses</title>
      <link>/courses/spring-15.md/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/courses/spring-15.md/</guid>
      <description>Course ID: ENAS 496 and 502 A study of stochastic processes and estimation, including fundamentals of detection and estimation. Vector space representation of random variables, Bayesian and Neyman-Pearson hypothesis testing, Bayesian and nonrandom parameter estimation, minimum-variance unbiased estimators, and the Cramer-Rao bound. Stochastic processes. Linear prediction and Kalman filtering. Poison counting process and renewal processes, Markov chains, branching processes, birth-death processes, and semi-Markov processes. Applications from communications, networking, and stochastic control.</description>
    </item>
    
    <item>
      <title>Creating a New Theme</title>
      <link>/posts/creating-a-new-theme/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/creating-a-new-theme/</guid>
      <description>Introduction This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I&amp;rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won&amp;rsquo;t cover using CSS to style your theme.
We&amp;rsquo;ll start with creating a new site with a very basic template.</description>
    </item>
    
    <item>
      <title>(Hu)go Template Primer</title>
      <link>/posts/hugo-template-primer/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/hugo-template-primer/</guid>
      <description>Hugo uses the excellent go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in go templates.
This document is a brief primer on using go templates.</description>
    </item>
    
    <item>
      <title>Getting Started with Hugo</title>
      <link>/posts/hugoisforlovers/</link>
      <pubDate>Wed, 02 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/hugoisforlovers/</guid>
      <description>Step 1. Install Hugo Goto hugo releases and download the appropriate version for your os and architecture.
Save it somewhere specific as we will be using it in the next step.
More complete instructions are available at installing hugo
Step 2. Build the Docs Hugo has its own example site which happens to also be the documentation site you are reading right now.
Follow the following steps:
Clone the hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands:</description>
    </item>
    
    <item>
      <title>Migrate to Hugo from Jekyll</title>
      <link>/posts/migrate-from-jekyll/</link>
      <pubDate>Mon, 10 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/posts/migrate-from-jekyll/</guid>
      <description>Move static content to static Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like
▾ &amp;lt;root&amp;gt;/ ▾ images/ logo.png should become
▾ &amp;lt;root&amp;gt;/ ▾ static/ ▾ images/ logo.png Additionally, you&amp;rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.</description>
    </item>
    
    <item>
      <title>Theoretical Challenges in Network Science</title>
      <link>/courses/fall-14.md/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/courses/fall-14.md/</guid>
      <description>Course ID: ENAS 962 This is an interdisciplinary course with a focus on the emerging science of complex networks and their mathematical models. Students learn about the recent research on the structure and analysis of such networks, and on models that abstract their basic properties. Topics include random graphs and their properties, probabilistic techniques for link analysis, centralized and decentralized search algorithms, random walks, diffusion and epidemic processes, and spectral methods.</description>
    </item>
    
    <item>
      <title>IEEE Data Storage Best Student Paper Award</title>
      <link>/news/ieee-15.md/</link>
      <pubDate>Thu, 12 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/news/ieee-15.md/</guid>
      <description>Our paper Noise-Enhanced Associative Memories received the IEEE Data Storage Best Student Paper Award.
IEEE Data Storage Best Student Paper Award for 2013
Awardees: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi and Lar R. Varshney
Paper: “Noise-Enhanced Associative Memories,” Proceedings of Neural Information Processing Systems, pp. 1682–1690, 2013. NIPS Proceedings
Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns.</description>
    </item>
    
    <item>
      <title>Graph-Based Information Processing: Scaling Laws and Applications</title>
      <link>/publications/thesis/karbasi-13e/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/karbasi-13e/</guid>
      <description>Abstract We live in a world characterized by massive information transfer and real-time communication. The demand for efficient yet low-complexity algorithms is widespread across different fields, including machine learning, signal processing and communications. Most of the problems that we encounter across these disciplines involves a large number of modules interacting with each other. It is therefore natural to represent these interactions and the ow of information between the modules in terms of a graph.</description>
    </item>
    
    <item>
      <title>About Hugo</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>This theme is created based on hugo-coder.
I made it possible to tell yourself more by my change.
Please see &amp;ldquo;FeaturesOfCoderPortfolio&amp;rdquo; in the post about the change. Regarding other demo contents, it is hugo-coder&amp;rsquo;s thing.
Have questions or suggestions? Feel free to open an issue on GitHub or ask me on Twitter.
Hugo is a static site engine written in Go.
It makes use of a variety of open source projects including:</description>
    </item>
    
    <item>
      <title>Amin Karbasi</title>
      <link>/people/amin-karbasi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/amin-karbasi/</guid>
      <description>Amin Karbasi is currently an associate professor of electrical engineering, computer science, and statistics and data science at Yale university. Prior to that he was a post-doctoral scholar at ETH Zurich, Switzerland (2013-2014). He obtained his Ph.D. (2012) and M.Sc. (2007) in computer and communication sciences from EPFL, Switzerland and his B.Sc. (2004) in electrical engineering from the same university.
Student Awards I am proudest of the recognitions my students/mentees received, including:</description>
    </item>
    
    <item>
      <title>Chris Harshaw</title>
      <link>/people/harshaw.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/harshaw.md/</guid>
      <description>Chris Harshaw is a 6th year PhD student in the Computer Science department where he is advised by Amin Karbasi and Dan Spielman. His research focuses on developing algorithmic techniques for causal inference, with the goal of enabling experimenters and analysts to be more confident in their findings. From an algorithmic perspective, he works on spectral algorithms, submodular optimization, and non-convex programming. More broadly, his interests lie in optimization, statistics, and their intersection.</description>
    </item>
    
    <item>
      <title>Chris Xu</title>
      <link>/people/xu.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/xu.md/</guid>
      <description>He is a first year PhD student in Statistics and Data Science. He received his bachelor&amp;rsquo;s degree in Mathematics and Computer Science from Courant Institute of Mathematical Sciences, NYU. He completed his Master’s degree in Mathematics at Courant Institute under the supervision of Professor Afonso S. Bandeira. His research interest lies broadly in the intersection of optimization, statistics, information theory, computation, and machine learning.</description>
    </item>
    
    <item>
      <title>Ehsan Kazemi</title>
      <link>/people/kazemi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/kazemi.md/</guid>
      <description>Ehsan Kazemi was Postdoc in the Department of Electrical Engineering and currently working in Google at Zürich.</description>
    </item>
    
    <item>
      <title>Farzin Haddadpour</title>
      <link>/people/haddadpour.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/haddadpour.md/</guid>
      <description>Farzin is a Postdoctoral Associate in the Institute for Network Science. He obtained his PhD from EECS department at Pennsylvania State University, working on the topic of the fault-tolerant distributed algorithms for machine learning problems. He is graduated from Sharif University of Technology respectively. Prior to joining Pennsylvania State University, he was research assistant at the Information Engineering Department of the Chinese University of Hong Kong. His research interests are broadly in the areas of information and coding theory and its application in machine learning and distributed computing.</description>
    </item>
    
    <item>
      <title>Felix Zhou</title>
      <link>/people/felix.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/felix.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Grigoris Velegkas</title>
      <link>/people/grigoris.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/grigoris.md/</guid>
      <description>I am Ph.D. student in computer science at Yale University. Before joing Yale I was at National Technical University of Athens.</description>
    </item>
    
    <item>
      <title>Hunala: A Personalized Risk Assessment Tool for Respiratory Illness</title>
      <link>/hunala/hunala.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/hunala/hunala.md/</guid>
      <description>Visit Hunala project to get informed your daily risk of getting infected. Hunala Gives a personalized daily assessment on your risk of contracting respiratory diseases, including COVID-19, updated daily. Helps you keep yourself and your community safe by keeping you informed about the spread of respiratory disease
What Hunala Does Gives a personalized daily assessment on your risk of contracting respiratory diseases, including COVID-19, updated daily. Helps you keep yourself and your community safe by keeping you informed about the spread of respiratory disease.</description>
    </item>
    
    <item>
      <title>Insu Han</title>
      <link>/people/han.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/han.md/</guid>
      <description>I am a Postdoctoral Associate in the Institute for Network Science. I obtained my Ph.D. degree in the School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST), where I am advised by Jinwoo Shin. I recieved an M.S. in Electrical engineering and a B.S. in Electrical Engineering and Mathematics (minored) from KAIST. My research interests focus on approximate algorithm design and analysis for large-scale machine learning and its applications.</description>
    </item>
    
    <item>
      <title>Jane Lee</title>
      <link>/people/jane.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/jane.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Javid Dadashkarimi</title>
      <link>/people/dadashkarimi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/dadashkarimi.md/</guid>
      <description>I’m Javid Dadashkarimi PhD candidate in computer science department at Yale University. I’m so grateful to work with Dustin Scheinost and Amin Karbasi in my PhD. Currently, I am interested in finding a mechanism to transfer human brain images between two atlases, with different resolutions, or with variable shapes: Check out my website for more information.
e
PhD, Computer Science Yale University
Advisor: Dustin Scheinost, Amin Karbasi
Thesis: Visualizing and Analysis of Functional Brain Connectomes</description>
    </item>
    
    <item>
      <title>Lin Chen</title>
      <link>/people/lin.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/lin.md/</guid>
      <description>Lin Chen is a postdoctoral scholar at the Simons Institute for the Theory of Computing, University of California, Berkeley . His research interests focus on machine learning theory. He was PhD student in the Department of Electrical Engineering. His research focuses on theoretical machine learning, including online optimization, submodular optimization, and adversarial robustness.
PhD Computer Science Yale University, New Haven, 2017-2020
Advisor: Professor Amin Karbasi Thesis: Online Optimization: Convex and Submodular Functions</description>
    </item>
    
    <item>
      <title>Mandy Singer</title>
      <link>/contact/contact.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/contact.md/</guid>
      <description>Mandy Singer is the administrative assistant for the Yale Institute for Network Science. Mandy has phenomenally varied experience, with talents as a repo-woman, cherry-picker operator, and is all-around competent. She has yet to be unable to do anything we have asked her to do.
Directions from Union Station to YINS building All you need is to drive the State St all the way up to Grove St and then turn left and follow up until Hillhouse Ave.</description>
    </item>
    
    <item>
      <title>Marko Mitrovic</title>
      <link>/people/marko.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/marko.md/</guid>
      <description>Marko was a Computer Science Ph.D. student advised by Amin Karbasi. His research focused on machine learning applications of submodularity. He is now at Google in Mountain View, California. His personal website can be found here.</description>
    </item>
    
    <item>
      <title>Mehraveh Salehi</title>
      <link>/people/salehi.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/salehi.md/</guid>
      <description>Mehraveh is currently Chief Experience Officer at Summary Analytics. She is interested in combinatorial optimization and machine learning algorithms with a focus on submodularity and its applications in the human brain. Her Ph.D. thesis focused on developing predictive models that relate human behavior to individual brain functional connectivity patterns as measured by fMRI. She enjoys playing Santour (a Persian music instrument), watching movies, and running.</description>
    </item>
    
    <item>
      <title>Mingrui Zhang</title>
      <link>/people/minguri.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/minguri.md/</guid>
      <description>Mingrui was a Ph.D. student in the Department of Statistics and Data Science. His research interest is theoretical machine learning and optimization, including online learning, submodular maximization, and projection-free optimization methods. Prior to joining Yale, he received his Bachelor&amp;rsquo;s degree in math and applied math from Peking University. He likes reading books and watching movies. He is also a supporter of Liverpool FC.</description>
    </item>
    
    <item>
      <title>Mohammad Shadravan</title>
      <link>/people/shadravan.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/shadravan.md/</guid>
      <description>Mohammad was a Postdoctoral Associate in the Institute for Network Science. He earned his PhD from Columbia University under the supervision of Professor Clifford Stein and Shipra Agrawal. His research interests lie primarily in the area of submodulariy in machine learning, algorithms for massive dataset, and optimization for sequential decision making. He completed his Master’s degree in the department of Combinatorics and Optimization at the University of Waterloo. Before that, he obtained his B.</description>
    </item>
    
    <item>
      <title>Outreach</title>
      <link>/outreach/outreach.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/outreach/outreach.md/</guid>
      <description>Tutorials ICML 2020 Tutorial on Submodular Optimization: From Discrete to Continuous and Back ISIT 2018 tutorial on “Submodularity in Information and Data Science”. Videos: Part One Slides: Part One, Part Two. CVPR 2018 tutorial “Big Data Summarization: Algorithms and Applications” in CVPR 2018. Videos: Part One, Part Two. Slides: Part One, Part Two. Workshops Amin Karbasi joint with Yaron Singer, Jeff A Bilmes, Andreas Krause, and Stefanie Jegelka, organized Discrete Structures in Machine Learning workshop at NIPS 2017.</description>
    </item>
    
    <item>
      <title>Peiyuan Zhang</title>
      <link>/people/peiyuan.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/peiyuan.md/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Siddharth Mitra</title>
      <link>/people/siddhart.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/siddhart.md/</guid>
      <description>I am a student of mathematics and computer science broadly interested in optimization, machine learning, and theoretical computer science. My recent interests surround online optimization and I have been working on designing adaptive algorithms for various online learning problems. I also have a soft spot for physics – I think it’s an all-round really fun subject and also provides tons of intuition.
In Fall 2020, I will be starting my PhD in CS at Yale.</description>
    </item>
    
    <item>
      <title>Software</title>
      <link>/software/software.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/software/software.md/</guid>
      <description>We highly believe in open and reproducible science. To this end, we try our best to publicly release our software and data sets.
Paper Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications:
Code is released here Paper: Streaming Weak Submodularity: Interpreting Neural Networks on the Fly
Code is released here. Paper: Probabilistic Submodular Maximization in Sub-Linear Time
Code is released here Paper: Differentially Submodular Maximization: Data Summarization in Disguise</description>
    </item>
    
    <item>
      <title>Yifei Min</title>
      <link>/people/yifei.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/people/yifei.md/</guid>
      <description>Yifei Min is a 2nd year PhD in Statistics and Data Science. His research interest is in theoretical machine learning and optimization. He likes tennis and movies.</description>
    </item>
    
  </channel>
</rss>
