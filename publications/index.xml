<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on IID Group</title>
    <link>/publications/</link>
    <description>Recent content in Publications on IID Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 01 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="/publications/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cross Atlas Remapping via Optimal Transport (CAROT): Creating connectomes for different atlases when raw data is not available</title>
      <link>/publications/2023/media/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/media/</guid>
      <description>Abstract Open-source, publicly available neuroimaging datasets—whether from large-scale data collection efforts or pooled from multiple smaller studies—offer unprecedented sample sizes and promote generalization efforts. Releasing data can democratize science, increase the replicability of findings, and lead to discoveries. Due to patient privacy and data storage concerns, researchers typically release preprocessed data with the voxelwise time series parcellated into a map of predefined regions, known as an atlas. However, releasing preprocessed data also limits the choices available to the end-user.</description>
    </item>
    
    <item>
      <title>Data-driven mapping between functional connectomes using optimal transport</title>
      <link>/publications/2021/javid-2021/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/javid-2021/</guid>
      <description>Abstract Functional connectomes derived from functional magnetic resonance imaging have long been used to understand the functional organization of the brain. Nevertheless, a connectome is intrinsically linked to the atlas used to create it. In other words, a connectome generated from one atlas is different in scale and resolution compared to a connectome generated from another atlas. Being able to map connectomes and derived results between different atlases without additional pre-processing is a crucial step in improving interpretation and generalization between studies that use different atlases.</description>
    </item>
    
    <item>
      <title>Batched Neural Bandits</title>
      <link>/publications/2021/gu-2021/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gu-2021/</guid>
      <description>Abstract In many sequential decision-making problems, the individuals are split into several batches and the decision-maker is only allowed to change her policy at the end of batches. These batch problems have a large number of applications, ranging from clinical trials to crowdsourcing. Motivated by this, we study the stochastic contextual bandit problem for general reward distributions under the batched setting. We propose the BatchNeuralUCB algorithm which combines neural networks with optimism to address the exploration-exploitation tradeoff while keeping the total number of batches limited.</description>
    </item>
    
    <item>
      <title>Safe Learning under Uncertain Objectives and Constraints</title>
      <link>/publications/2020/karbasi-2020/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020/</guid>
      <description>Abstract In this paper, we consider non-convex optimization problems under\textit {unknown} yet safety-critical constraints. Such problems naturally arise in a variety of domains including robotics, manufacturing, and medical procedures, where it is infeasible to know or identify all the constraints. Therefore, the parameter space should be explored in a conservative way to ensure that none of the constraints are violated during the optimization process once we start from a safe initialization point.</description>
    </item>
    
    <item>
      <title>Batched Multi-Armed Bandits with Optimal Regret</title>
      <link>/publications/2019/esfandiari-2019a/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/esfandiari-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1−1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1−1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1−1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2019/lin-2019d/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Θ(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask.</title>
      <link>/publications/2018/ghili-18a/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/ghili-18a/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Deletion-Robust Submodular Maximization at Scale.</title>
      <link>/publications/2017/kazemi-17a/</link>
      <pubDate>Tue, 03 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/kazemi-17a/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation. We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting</title>
      <link>/publications/2016/lin-16d/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16d/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Adaptive Content Search Through Comparisons</title>
      <link>/publications/2011/karbasi-11c/</link>
      <pubDate>Mon, 03 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11c/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner: the user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>An Estimation Theoretic Approach for Sparsity Pattern Recovery in the Noisy Setting</title>
      <link>/publications/2009/hormati-09a/</link>
      <pubDate>Sat, 03 Jan 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/hormati-09a/</guid>
      <description>Abstract Compressed sensing deals with the reconstruction of sparse signals using a small number of linear measurements. One of the main challenges in compressed sensing is to find the support of a sparse signal. In the literature, several bounds on the scaling law of the number of measurements for successful support recovery have been derived where the main focus is on random Gaussian measurement matrices. In this paper, we investigate the noisy support recovery problem from an estimation theoretic point of view, where no specific assumption is made on the underlying measurement matrix.</description>
    </item>
    
    <item>
      <title>Replicability in Reinforcement Learning</title>
      <link>/publications/2023/neurips1/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/neurips1/</guid>
      <description>Abstract We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same.</description>
    </item>
    
    <item>
      <title>Replicable Clustering</title>
      <link>/publications/2023/neurips2/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/neurips2/</guid>
      <description>Abstract We design replicable algorithms in the context of statistical clustering under the recently introduced notion of replicability from Impagliazzo et al. [2022]. According to this definition, a clustering algorithm is replicable if, with high probability, its output induces the exact same partition of the sample space after two executions on different inputs drawn from the same distribution, when its internal randomness is shared across the executions. We propose such algorithms for the statistical k-medians, statistical k-means, and statistical k-centers problems by utilizing approximation routines for their combinatorial counterparts in a black-box manner.</description>
    </item>
    
    <item>
      <title>Accelerating Transformers via Kernel Density Estimation</title>
      <link>/publications/2023/icml-1/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-1/</guid>
      <description>Abstract Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, naïve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation (KDE) problem, and an efficient KDE solver can be further utilized to accelerate the latter via subsampling-based fast matrix products.</description>
    </item>
    
    <item>
      <title>Approximate Thompson Sampling with Logarithmic Batches: Bandits and Reinforcement Learning</title>
      <link>/publications/2023/icml-3/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-3/</guid>
      <description>Abstract TBC</description>
    </item>
    
    <item>
      <title>Statistical Indistinguishability of Learning Algorithms</title>
      <link>/publications/2023/icml-2/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/icml-2/</guid>
      <description>Abstract TBC</description>
    </item>
    
    <item>
      <title>Stacking multiple optimal transport policies to map functional connectomes</title>
      <link>/publications/2023/ciss-20223/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/ciss-20223/</guid>
      <description>Abstract Connectomics is a popular approach for understanding the brain with neuroimaging data. However, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from another. Consequently, connectomes generated from different atlases cannot be used in the same analysis. This limitation hinders efforts toward increasing sample size and demonstrating generalizability across datasets. Recently, we proposed Cross Atlas Remapping via Optimal Transport (CAROT) to find a spatial mapping between a pair of atlases based on a set of training data.</description>
    </item>
    
    <item>
      <title>Exact Gradient Computation for Spiking Neural Networks Through Forward Propagation</title>
      <link>/publications/2023/aistats-2022a/</link>
      <pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/aistats-2022a/</guid>
      <description>Abstract Spiking neural networks (SNN) have recently emerged as alternatives to traditional neural networks, owing to energy efficiency benefits and capacity to better capture biological neuronal mechanisms. However, the classic backpropagation algorithm for training traditional networks has been notoriously difficult to apply to SNN due to the hard-thresholding and discontinuities at spike times. Therefore, a large majority of prior work believes exact gradients for SNN w.r.t. their weights do not exist and has focused on approximation methods to produce surrogate gradients.</description>
    </item>
    
    <item>
      <title>Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD</title>
      <link>/publications/2023/iclar-2022b/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/iclar-2022b/</guid>
      <description>Abstract We provide sharp path-dependent generalization and excess risk guarantees for the full-batch Gradient Descent (GD) algorithm on smooth losses (possibly non-Lipschitz, possibly nonconvex). At the heart of our analysis is an upper bound on the generalization error, which implies that average output stability and a bounded expected optimization error at termination lead to generalization. This result shows that a small generalization error occurs along the optimization path, and allows us to bypass Lipschitz or sub-Gaussian assumptions on the loss prevalent in previous works.</description>
    </item>
    
    <item>
      <title>Reproducible Bandits</title>
      <link>/publications/2023/iclar-2022a/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/iclar-2022a/</guid>
      <description>Abstract In this paper, we introduce the notion of reproducible policies in the context of stochastic bandits, one of the canonical problems in interactive learning. A policy in the bandit environment is called reproducible if it pulls, with high probability, the \emph{exact} same sequence of arms in two different and independent executions (i.e., under independent reward realizations). We show that not only do reproducible policies exist, but also they achieve almost the same optimal (non-reproducible) regret bounds in terms of the time horizon.</description>
    </item>
    
    <item>
      <title>Black-Box Generalization</title>
      <link>/publications/2022/neurips-4/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-4/</guid>
      <description>Abstract We provide the first generalization error analysis for black-box learning through derivative-free optimization. Under the assumption of a Lipschitz and smooth unknown loss, we consider the Zeroth-order Stochastic Search (ZoSS) algorithm, that updates a d-dimensional model by replacing stochastic gradient directions with stochastic differences of K+1 perturbed loss evaluations per dataset (example) query. For both unbounded and bounded possibly nonconvex losses, we present the first generalization bounds for the ZoSS algorithm.</description>
    </item>
    
    <item>
      <title>Fast Neural Kernel Embeddings for General Activations</title>
      <link>/publications/2022/neurips-6/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-6/</guid>
      <description>Abstract Infinite width limit has shed light on generalization and optimization aspects of deep learning by establishing connections between neural networks and kernel methods. Despite their importance, the utility of these kernel methods was limited in large-scale learning settings due to their (super-)quadratic runtime and memory complexities. Moreover, most prior works on neural kernels have focused on the ReLU activation, mainly due to its popularity but also due to the difficulty of computing such kernels for general activations.</description>
    </item>
    
    <item>
      <title>Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes </title>
      <link>/publications/2022/neurips-3/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-3/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>On Optimal Learning Under Targeted Data Poisoning </title>
      <link>/publications/2022/neurips-5/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-5/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Submodular Maximization in Clean Linear Time</title>
      <link>/publications/2022/li-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/li-2022/</guid>
      <description>Abstract In this paper, we provide the first deterministic algorithm that achieves the tight 1−1/e approximation guarantee for submodular maximization under a cardinality (size) constraint while making a number of queries that scales only linearly with the size of the ground set n. To complement our result, we also show strong information-theoretic lower bounds. More specifically, we show that when the maximum cardinality allowed for a solution is constant, no algorithm making a sub-linear number of function evaluations can guarantee any constant approximation ratio.</description>
    </item>
    
    <item>
      <title>The Best of Both Worlds: Reinforcement Learning with Logarithmic Regret and Policy Switches</title>
      <link>/publications/2022/icml2-2022/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml2-2022/</guid>
      <description>Abstract In this paper, we study the problem of regret minimization for episodic Reinforcement Learning (RL) both in the model-free and the model-based setting. We focus on learning with general function classes and general model classes, and we derive results that scale with the eluder dimension of these classes. In contrast to the existing body of work that mainly establishes instance-independent regret guarantees, we focus on the instance-dependent setting and show that the regret scales logarithmically with the horizon T, provided that there is a gap between the best and the second best action in every state.</description>
    </item>
    
    <item>
      <title>Universal Rates for Interactive Learning</title>
      <link>/publications/2022/neurips-7/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/neurips-7/</guid>
      <description>Abstract </description>
    </item>
    
    <item>
      <title>Self-Consistency of the Fokker-Planck Equation</title>
      <link>/publications/2022/karbasi-2022b/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022b/</guid>
      <description>Abstract The Fokker-Planck equation (FPE) is the partial differential equation that governs the density evolution of the Itô process and is of great importance to the literature of statistical physics and machine learning. The FPE can be regarded as a continuity equation where the change of the density is completely determined by a time varying velocity field. Importantly, this velocity field also depends on the current density function. As a result, the ground-truth velocity field can be shown to be the solution of a fixed-point equation, a property that we call self-consistency.</description>
    </item>
    
    <item>
      <title>Scalable MCMC Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/icml3-2022/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/icml3-2022/</guid>
      <description>Abstract A determinantal point process (DPP) is an elegant model that assigns a probability to every subset of a collection of items. While conventionally a DPP is parameterized by a symmetric kernel matrix, removing this symmetry constraint, resulting in nonsymmetric DPPs (NDPPs), leads to significant improvements in modeling power and predictive performance. Recent work has studied an approximate Markov chain Monte Carlo (MCMC) sampling algorithm for NDPPs restricted to size- subsets (called -NDPPs).</description>
    </item>
    
    <item>
      <title>Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport</title>
      <link>/publications/2022/miccai-2022/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/miccai-2022/</guid>
      <description>Abstract Connectomics is a popular approach for understanding the brain with neuroimaging data. Yet, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from another atlas. These differences hinder interpreting, generalizing, and combining connectomes and downstream results from different atlases. Recently, it was proposed that a mapping between atlases can be estimated such that connectomes from one atlas (\textit{i.e.}, source atlas) can be reconstructed into a connectome from a different atlas (\textit{i.</description>
    </item>
    
    <item>
      <title>Learning Distributionally Robust Models at Scale via Composite Optimization</title>
      <link>/publications/2022/iclar-2022a/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022a/</guid>
      <description>Abstract To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples– which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods.</description>
    </item>
    
    <item>
      <title>Scalable Sampling for Nonsymmetric Determinantal Point Processes</title>
      <link>/publications/2022/iclar-2022b/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/iclar-2022b/</guid>
      <description>Abstract A determinantal point process (DPP) on a collection of M items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items. Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well.</description>
    </item>
    
    <item>
      <title>Federated Functional Gradient Boosting</title>
      <link>/publications/2022/karbasi-2022a/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/karbasi-2022a/</guid>
      <description>Abstract In this paper, we initiate a study of functional minimization in Federated Learning. First, in the semi-heterogeneous setting, when the marginal distributions of the feature vectors on client machines are identical, we develop the federated functional gradient boosting (FFGB) method that provably converges to the global minimum. Subsequently, we extend our results to the fully-heterogeneous setting (where marginal distributions of feature vectors may differ) by designing an efficient variant of FFGB called FFGB.</description>
    </item>
    
    <item>
      <title>An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks</title>
      <link>/publications/2021/neurips3/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips3/</guid>
      <description>Abstract It is well known that modern deep neural networks are powerful enough to memorize datasets even when the labels have been randomized. Recently, Vershynin (2020) settled a long standing question by Baum (1988), proving that \emph{deep threshold} networks can memorize n points in d dimensions using ˜(e1/δ2+n‾√) neurons and ˜(e1/δ2(d+n‾√)+n) weights, where δ is the minimum distance between the points. In this work, we improve the dependence on δ from exponential to almost linear, proving that ˜(1δ+n‾√) neurons and ˜(dδ+n) weights are sufficient.</description>
    </item>
    
    <item>
      <title>Multiple Descent: Design Your Own Generalization Curve</title>
      <link>/publications/2021/neurips2/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips2/</guid>
      <description>Abstract This paper explores the generalization loss of linear regression in variably parameter- ized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.</description>
    </item>
    
    <item>
      <title>Parallelizing Thompson Sampling</title>
      <link>/publications/2021/neurips4/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips4/</guid>
      <description>Abstract How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon T , our batch Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T ) batch queries.</description>
    </item>
    
    <item>
      <title>Submodular &#43; Concave</title>
      <link>/publications/2021/neurips1/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/neurips1/</guid>
      <description>Abstract It has been well established that first order optimization methods can converge to the maximal objective value of concave functions and provide constant factor approximation guarantees for (non-convex/non-concave) continuous submodular functions. In this work, we initiate the study of the maximization of functions of the form F (x) = G(x) + C (x) over a solvable convex body P, where G is a smooth DR-submodular function and C is a smooth concave function.</description>
    </item>
    
    <item>
      <title>Regularized Submodular Maximization at Scale</title>
      <link>/publications/2021/karbasi-2020j/</link>
      <pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020j/</guid>
      <description>Abstract In this paper, we propose scalable methods for maximizing a regularized submodular function f(·) = g(·) − l(·) expressed as the difference between a monotone submodular function g and a modular function l. Indeed, submodularity is inherently related to the notions of diversity, coverage, and representativeness. In particular, finding the mode (i.e., the most likely configuration) of many popular probabilistic models of diversity, such as determinantal point processes, submodular probabilistic models, and strongly log-concave distributions, involves maximization of (regularized) submodular functions.</description>
    </item>
    
    <item>
      <title>Adaptivity in Adaptive Submodularity</title>
      <link>/publications/2021/esfandiari-2019b/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2019b/</guid>
      <description>Abstract Adaptive sequential decision making is one of the central challenges in machine learning and artificial intelligence. In such problems, the goal is to design an interactive policy that plans for an action to take, from a finite set of n actions, given some partial observations. It has been shown that in many applications such as active learning, robotics, sequential experimental design, and active detection, the utility function satisfies adaptive submodularity, a notion that generalizes the notion of diminishing returns to policies.</description>
    </item>
    
    <item>
      <title>Learning and Certification under Instance-targeted Poisoning</title>
      <link>/publications/2021/gao-21/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/gao-21/</guid>
      <description>Abstract In this paper, we study PAC learnability and certification under instance-targeted poisoning attacks, where the adversary may change a fraction of the training set with the goal of fooling the learner at a specific target instance. Our first contribution is to formalize the problem in various settings, and explicitly discussing subtle aspects such as learner&amp;rsquo;s randomness and whether (or not) adversary&amp;rsquo;s attack can depend on it. We show that when the budget of the adversary scales sublinearly with the sample complexity, PAC learnability and certification are achievable.</description>
    </item>
    
    <item>
      <title>The curious case of adversarially robust models: More data can help, double descend, or hurt generalization</title>
      <link>/publications/2021/karbasi-2020g/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020g/</guid>
      <description>Abstract Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems.</description>
    </item>
    
    <item>
      <title>Meta Learning in the Continuous Time Limit</title>
      <link>/publications/2021/karbasi-2020c/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/karbasi-2020c/</guid>
      <description>Abstract In this paper, we establish the ordinary differential equation (ode) that underlies the train- ing dynamics of Model-Agnostic Meta-Learning (maml). Our continuous-time limit view of the process eliminates the influence of the manually chosen step size of gradient descent and includes the existing gradient descent training algorithm as a special case that results from a specific discretization. We show that the maml ode enjoys a linear convergence rate to an approximate stationary point of the maml loss function for strongly convex task losses, even when the corresponding maml loss is non-convex.</description>
    </item>
    
    <item>
      <title>Regret Bounds for Batched Bandits</title>
      <link>/publications/2021/esfandiari-2021/</link>
      <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/esfandiari-2021/</guid>
      <description>Abstract We present simple and efficient algorithms for the batched stochastic multi-armed bandit and batched stochastic linear bandit problems. We prove bounds for their expected regrets that improve over the best known regret bounds for any number of batches. In particular, our algorithms in both settings achieve the optimal expected regrets by using only a logarithmic number of batches. We also study the batched adversarial multi-armed bandit problem for the first time and find the optimal regret, up to logarithmic factors, of any algorithm with predetermined batch sizes.</description>
    </item>
    
    <item>
      <title>Continuous Submodular Maximization: Beyond DR-Submodularity</title>
      <link>/publications/2020/karbasi-2020b/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020b/</guid>
      <description>Abstract n this paper, we propose the first continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called Coordinate-Ascent+, achieves $\frac{e-1}{2e-1}-\epsilon$ a -approximation guarantee while performing $O(n/\epsilon)$ iterations, where the computational complexity of each iteration is roughly $O(n/\sqrt(\epsilon)+n\log n)$(here $n$, denotes the dimension of the optimization problem).</description>
    </item>
    
    <item>
      <title>Minimax Regret of Switching-Constrained Online Convex Optimization: No Phase Transition</title>
      <link>/publications/2020/lin-2019d/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2019d/</guid>
      <description>Abstract We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. We here show that T-round switching-constrained OCO with fewer than K switches has a minimax regret of $Θ(\frac{T}{\sqrt{K}})$.</description>
    </item>
    
    <item>
      <title>Online MAP Inference of Determinantal Point Processes</title>
      <link>/publications/2020/bhaskara-2020/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/bhaskara-2020/</guid>
      <description>Abstract In this paper, we provide an efficient approximation algorithm for finding the most likelihood configuration (MAP) of size k for Determinantal Point Processes (DPP) in the online setting where the data points arrive in an arbitrary order and the algorithm cannot discard the selected elements from its local memory. Given a tolerance additive error $\eta$, our onlinealgorithm achieves a $k^{O(k)}$ multiplicative approximation guarantee with an additive error $\eta$, using a memory footprint independent of the size of the data stream.</description>
    </item>
    
    <item>
      <title>Submodular Maximization Through Barrier Functions</title>
      <link>/publications/2020/karbasi-2020i/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020i/</guid>
      <description>Abstract In this paper, we introduce a novel technique for constrained submodular maximization, inspired by barrier functions in continuous optimization. This connection not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a k-matchoid and l-knapsack constraint (for l ≤ k), we propose a potential function that can be approximately minimized.</description>
    </item>
    
    <item>
      <title>More data can expand the generalization gap between adversarially robust and standard models</title>
      <link>/publications/2020/karbasi-2020h/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020h/</guid>
      <description>Abstract Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models.</description>
    </item>
    
    <item>
      <title>Streaming Submodular Maximization under a k-Set System Constraint. </title>
      <link>/publications/2020/haba-2020/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/haba-2020/</guid>
      <description>Abstract In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a k-matchoid constraint. Moreover, we propose the first streaming algorithm for monotone submodular maximization subject to k-extendible and k-set system constraints. Together with our proposed reduction, we obtain O(klogk) and O(k2logk) approximation ratio for submodular maximization subject to the above constraints, respectively.</description>
    </item>
    
    <item>
      <title>Black Box Submodular Maximization: Discrete and Continuous Settings</title>
      <link>/publications/2020/lin-2020a/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/lin-2020a/</guid>
      <description>Abstract In this paper, we consider the problem of black box continuous submodular maximization where we only have access to the function values and no information about the derivatives is provided. For a monotone and continuous DR-submodular function, and subject to a bounded convex body constraint, we propose Black-box Continuous Greedy, a derivative-free algorithm that provably achieves the tight [(1−1/e)OPT−ϵ] approximation guarantee with $O(d/ϵ^3)$ function evaluations. We then extend our result to the stochastic setting where function values are subject to stochastic zero-mean noise.</description>
    </item>
    
    <item>
      <title>One Sample Stochastic Frank-Wolfe</title>
      <link>/publications/2020/mingrui-2020c/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mingrui-2020c/</guid>
      <description>Abstract One of the beauties of the projected gradient descent method lies in its rather simple mechanism and yet stable behavior with inexact, stochastic gradients, which has led to its wide-spread use in many machine learning applications. However, once we replace the projection operator with a simpler linear program, as is done in the Frank-Wolfe method, both simplicity and stability take a serious hit. The aim of this paper is to bring them back without sacrificing the efficiency.</description>
    </item>
    
    <item>
      <title>Quantized frank-wolfe: Faster optimization, lower communication, and projection free</title>
      <link>/publications/2020/karbasi-2020f/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020f/</guid>
      <description>Abstract How can we efficiently mitigate the overhead of gradient communications in distributed optimization? This problem is at the heart of training scalable machine learning models and has been mainly studied in the unconstrained setting. In this paper, we propose Quantised Frank-Wolfe (QFW), the first projection free and communication-efficient algorithm for solving constrained optimization problems at scale. We consider both convex and non-convex objective functions, expressed as a finite-sum or more generally a stochastic optimization problem, and provide strong theoretical guarantees on the convergence rate of QFW.</description>
    </item>
    
    <item>
      <title>Adaptive Sequence Submodularity</title>
      <link>/publications/2019/marko-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/marko-2019a/</guid>
      <description>Abstract In many machine learning applications, one needs to interactively select a sequence of items (e.g., recommending movies based on a user&amp;rsquo;s feedback) or make sequential decisions in a certain order (e.g., guiding an agent through a series of states). Not only do sequences already pose a dauntingly large search space, but we must also take into account past observations, as well as the uncertainty of future outcomes. Without further structure, finding an optimal sequence is notoriously challenging, if not completely intractable.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization: From Full-Information to Bandit Feedback</title>
      <link>/publications/2019/minguri-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/minguri-2019a/</guid>
      <description>Abstract In this paper, we propose three online algorithms for submodular maximization. The first one, Mono-Frank-Wolfe, reduces the number of per-function gradient evaluations from $T^{1/2}$ [Chen2018Online] and $T^{3/2}$ [chen2018projection] to 1, and achieves a (1−1/e)-regret bound of $O(T^{4/5})$. The second one, Bandit-Frank-Wolfe, is the first bandit algorithm for continuous DR-submodular maximization, which achieves a (1−1/e)-regret bound of $O(T^{8/9})$. Finally, we extend Bandit-Frank-Wolfe to a bandit algorithm for discrete submodular maximization, Responsive-Frank-Wolfe, which attains a (1−1/e)-regret bound of $O(T^{8/9})$ in the responsive bandit setting.</description>
    </item>
    
    <item>
      <title>Stochastic Continuous Greedy &#43;&#43;: When Upper and Lower Bounds Match</title>
      <link>/publications/2019/karbasi-2019a/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/karbasi-2019a/</guid>
      <description>Abstract In this paper, we develop $\scg~(\text{SCG}{++})$, the first efficient variant of a conditional gradient method for maximizing a continuous submodular function subject to a convex constraint. Concretely, for a monotone and continuous DR-submodular function, \SCGPP achieves a tight [(1−1/e)\OPT−ϵ] solution while using $O(1/ϵ^2)$ stochastic gradients and O(1/ϵ) calls to the linear optimization oracle. The best previously known algorithms either achieve a suboptimal $[(1/2)\OPT−ϵ]$ solution with $O(1/ϵ^2)$ stochastic gradients or the tight $[(1−1/e)\OPT−ϵ]$ solution with suboptimal $O(1/ϵ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications</title>
      <link>/publications/2019/harshaw-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/harshaw-2019a/</guid>
      <description>Abstract It is generally believed that submodular functions–and the more general class of γ -weakly submodular functions–may only be optimized under the non-negativity assumption f(S)≥0 . In this paper, we show that once the function is expressed as the difference f=g−c , where g is monotone, non-negative, and γ -weakly submodular and c is non-negative modular, then strong approximation guarantees may be obtained. We present an algorithm for maximizing g−c under a k -cardinality constraint which produces a random feasible set S such that $𝔼[g(S)−c(S)]\geq (1−e−γ−\epsilon)g(OPT)−c(OPT)$ , whose running time is $O(\frac{n}{\epsilon}log2\frac{1}{\epsilon})$ , independent of k .</description>
    </item>
    
    <item>
      <title>Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity. </title>
      <link>/publications/2019/kazemi-2019a/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/kazemi-2019a/</guid>
      <description>Abstract Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular function in the streaming setting with a cardinality constraint k . We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only O(k) elements and achieves the tight $\frac{1}{2}$ -approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal $\frac{1}{4}$ -approximation with Θ(k) memory or the optimal 12 -approximation with O(klogk) memory.</description>
    </item>
    
    <item>
      <title>Projection-Free Bandit Convex Optimization</title>
      <link>/publications/2019/lin-2019a/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019a/</guid>
      <description>Abstract In this paper, we propose the first computationally efficient projection-free algorithm for bandit convex optimization (BCO) with a general convex constraint. We show that our algorithm achieves a sublinear regret of $O(nT^{4/5})$ (where T is the horizon and n is the dimension) for any bounded convex functions with uniformly bounded gradients. We also evaluate the performance of our algorithm against baselines on both synthetic and real data sets for quadratic programming, portfolio selection and matrix completion problems.</description>
    </item>
    
    <item>
      <title>Eliminating Latent Discrimination: Train Then Mask</title>
      <link>/publications/2019/ghili-2019/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/ghili-2019/</guid>
      <description>Abstract How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making.</description>
    </item>
    
    <item>
      <title>Unconstrained submodular maximization with constant adaptive complexity</title>
      <link>/publications/2019/lin-2019b/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/2019/lin-2019b/</guid>
      <description>Abstract In this paper, we consider the unconstrained submodular maximization problem. We propose the first algorithm for this problem that achieves a tight $(1/2−ε)$-approximation guarantee using $O(ε^{−1})$ adaptive rounds and a linear number of function evaluations. No previously known algorithm for this problem achieves an approximation ratio better than 1/3 using less than Ω(n) rounds of adaptivity, where n is the size of the ground set. Moreover, our algorithm easily extends to the maximization of a non-negative continuous DR-submodular function subject to a box constraint, and achieves a tight (1/2−ε)-approximation guarantee for this problem while keeping the same adaptive and query complexities.</description>
    </item>
    
    <item>
      <title>Do Less, Get More: Streaming Submodular Maximization with Subsampling</title>
      <link>/publications/2018/feldman-18a/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/feldman-18a/</guid>
      <description>Abstract In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint).</description>
    </item>
    
    <item>
      <title>Data Summarization at Scale: A Two-Stage Submodular Approach</title>
      <link>/publications/2018/marko-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18b/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summariza- tion tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training func- tions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings.</title>
      <link>/publications/2018/mokhtari-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18b/</guid>
      <description>Abstract In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1−1/e) approximation factor of the optimum global solution using only local computation and communication.</description>
    </item>
    
    <item>
      <title>Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity.</title>
      <link>/publications/2018/lin-18c/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18c/</guid>
      <description>Abstract Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized.</description>
    </item>
    
    <item>
      <title>Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints.</title>
      <link>/publications/2018/kazemi-2018b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018b/</guid>
      <description>Abstract Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions.</description>
    </item>
    
    <item>
      <title>Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?</title>
      <link>/publications/2018/lin-18b/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18b/</guid>
      <description>Abstract Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for functions obeying properties that generalize submodularity, has been the focus of several recent works.</description>
    </item>
    
    <item>
      <title>Comparison Based Learning from Weak Oracles</title>
      <link>/publications/2018/kazemi-2018a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/kazemi-2018a/</guid>
      <description>Abstract There is increasing interest in learning algorithms that involve interaction between hu- man and machine. Comparison-based queries are among the most natural ways to get feed- back from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query.</description>
    </item>
    
    <item>
      <title>Conditional Gradient Method for Stochastic Submodular Maximization: Closing the Gap</title>
      <link>/publications/2018/mokhtari-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mokhtari-18a/</guid>
      <description>Abstract In this paper, we study the problem of constrained and stochastic continuous submodular maximization. Even though the objective function is not concave (nor convex) and is defined in terms of an expectation, we develop a variant of the conditional gradient method, called Stochastic Continuous Greedy, which achieves a tight approximation guarantee. More precisely, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that Stochastic Continuous Greedy achieves a $[(1−1/e)OPT−\eps]$ guarantee (in expectation) with $O(1/\eps^3)$ stochastic gradient computations.</description>
    </item>
    
    <item>
      <title>Online Continuous Submodular Maximization</title>
      <link>/publications/2018/lin-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/lin-18a/</guid>
      <description>Abstract In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of $O(\sqrt(T))$ (where T is the horizon of the online optimization problem) against a (1−1/e) -approximation to the best feasible solution in hindsight.</description>
    </item>
    
    <item>
      <title>Submodularity on Hypergraphs: From Sets to Sequences</title>
      <link>/publications/2018/marko-18a/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/marko-18a/</guid>
      <description>Abstract The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set.</description>
    </item>
    
    <item>
      <title>Gradient Methods for Submodular Maximization</title>
      <link>/publications/2017/hassani-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/hassani-17a/</guid>
      <description>Abstract In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima.</description>
    </item>
    
    <item>
      <title>Interactive Submodular Bandit.</title>
      <link>/publications/2017/lin-17b/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17b/</guid>
      <description>Abstract In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible.</description>
    </item>
    
    <item>
      <title>Streaming Weak Submodularity: Interpreting Neural Networks on the Fly.</title>
      <link>/publications/2017/elenberg-17a/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/elenberg-17a/</guid>
      <description>Abstract In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function.</description>
    </item>
    
    <item>
      <title>Differentially Private Submodular Maximization: Data Summarization in Disguise.</title>
      <link>/publications/2017/marko-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/marko-17a/</guid>
      <description>Abstract Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/baharan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/baharan-17a/</guid>
      <description>Abstract How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution.</description>
    </item>
    
    <item>
      <title>Probabilistic Submodular Maximization in Sub-Linear Time.</title>
      <link>/publications/2017/stan-17a/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/stan-17a/</guid>
      <description>Abstract In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.</description>
    </item>
    
    <item>
      <title>Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting.</title>
      <link>/publications/2017/lin-17a/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/lin-17a/</guid>
      <description>Abstract In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.</description>
    </item>
    
    <item>
      <title>Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization.</title>
      <link>/publications/2017/moran-17a/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/moran-17a/</guid>
      <description>Abstract It is known that greedy methods perform well for maximizing \textitmonotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show—arguably, surprisingly—that invoking the classical greedy algorithm $O(\sqrt(k))$ -times leads to the (currently) fastest deterministic algorithm, called RepeatedGreedy, for maximizing a general submodular function subject to k -independent system constraints. RepeatedGreedy achieves (1+O(1/$\sqrt(k)$))k approximation using $O(nr\sqrt{k})$ function evaluations (here, n and r denote the size of the ground set and the maximum size of a feasible solution, respectively).</description>
    </item>
    
    <item>
      <title>A Submodular Approach to Create Individualized Parcellations of the Human Brain</title>
      <link>/publications/2017/mehraveh-17a/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/publications/2017/mehraveh-17a/</guid>
      <description>Abstract Recent studies on functional neuroimaging (e.g. fMRI) attempt to model the brain as a network. A conventional functional connectivity approach for defining nodes in the network is grouping similar voxels together, a method known as functional parcellation. The majority of previous work on human brain parcellation employs a group-level analysis by collapsing data from the entire population. However, these methods ignore the large amount of inter-individual variability and uniqueness in connectivity.</description>
    </item>
    
    <item>
      <title>Estimating the Size of a Large Network and its Communities from a Random Sample</title>
      <link>/publications/2016/lin-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16c/</guid>
      <description>Abstract Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/lin-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>Seeing the Unseen Network: Inferring Hidden Social Ties from Respondent-Driven Sampling.</title>
      <link>/publications/2016/lin-16a/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16a/</guid>
      <description>Abstract Learning about the social structure of hidden and hard-to-reach populations — such as drug users and sex workers — is a major goal of epidemiological and public health research on risk behaviors and disease prevention. Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network. In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents.</description>
    </item>
    
    <item>
      <title>Submodular Variational Inference for Network Reconstruction</title>
      <link>/publications/2016/lin-16e/</link>
      <pubDate>Sun, 03 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/lin-16e/</guid>
      <description>Abstract In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions (e.g. phone calls, text messages, social media posts) may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. The process only traverses and thus reveals a limited portion of the edges. The network reconstruction/inference problem is to infer the unrevealed connections.</description>
    </item>
    
    <item>
      <title>Learning network structures from firing patterns.</title>
      <link>/publications/2016/amin-16a/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/amin-16a/</guid>
      <description>Abstract How can we decipher the hidden structure of a network based on limited observations? This question arises in many scenarios ranging from social to wireless and to neural networks. In such settings, we typically observe the nodes&amp;rsquo; behaviors (e.g., the time a node learns about a piece of information, or the time a node gets infected by a disease), and we are interested in inferring the true network over which the diffusion takes place.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Cover: Succinctly Summarizing Massive Data. </title>
      <link>/publications/2015/baharan-15b/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15b/</guid>
      <description>Abstract How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition preva- lent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution.</description>
    </item>
    
    <item>
      <title>Lazier Than Lazy Greedy.</title>
      <link>/publications/2015/baharan-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/baharan-15a/</guid>
      <description>Abstract Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a (1 − 1/e − ε) approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint.</description>
    </item>
    
    <item>
      <title>Submodular Surrogates for Value of Information</title>
      <link>/publications/2015/chen-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15a/</guid>
      <description>Abstract How should we gather information to make effective decisions? A classical answer to this fundamental problem is given by the decision-theoretic value of information. Unfortunately, optimizing this objective is intractable, and myopic (greedy) approximations are known to perform poorly. In this paper, we introduce DiRECt, an efficient yet near-optimal algorithm for nonmyopically optimizing value of information. Crucially, DiRECt uses a novel surrogate objective that is: (1) aligned with the value of information problem (2) efficient to evaluate and (3) adaptive submodular.</description>
    </item>
    
    <item>
      <title>Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning.</title>
      <link>/publications/2015/lucic-15a/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/lucic-15a/</guid>
      <description>Abstract Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry.</description>
    </item>
    
    <item>
      <title>Fast Mixing for Discrete Point Processes</title>
      <link>/publications/2015/rebeschini-15a/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/rebeschini-15a/</guid>
      <description>Abstract We investigate the systematic mechanism for designing fast mixing Markov chain Monte Carlo algorithms to sample from discrete point processes under the Dobrushin uniqueness condition for Gibbs measures. Discrete point processes are defined as probability distributions μ(S)∝\exp(βf(S)) over all subsets $S\in 2^V$ of a finite set V through a bounded set function $f:2^V→\mathbb{R}$ and a parameter β&amp;gt;0. A subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage.</description>
    </item>
    
    <item>
      <title>Sequential Information Maximization: When is Greedy Near-optimal?</title>
      <link>/publications/2015/chen-15b/</link>
      <pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/chen-15b/</guid>
      <description>Abstract Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantifies the usefulness of observations is Shannon’s mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations.</description>
    </item>
    
    <item>
      <title>Normalization Phenomena in Asynchronous Networks.</title>
      <link>/publications/2015/karbasi-15b/</link>
      <pubDate>Sun, 03 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15b/</guid>
      <description>Abstract In this work we study a diffusion process in a network that consists of two types of vertices: inhibitory vertices (those obstructing the diffusion) and excitatory vertices (those facilitating the diffusion). We consider a continuous time model in which every edge of the network draws its transmission time randomly. For such an asynchronous diffusion process it has been recently proven that in Erdős-Rényi random graphs a normalization phenomenon arises: whenever the diffusion starts from a large enough (but still tiny) set of active vertices, it only percolates to a certain level that depends only on the activation threshold and the ratio of inhibitory to excitatory vertices.</description>
    </item>
    
    <item>
      <title>Asynchronous decoding of LDPC codes over BEC.</title>
      <link>/publications/2015/haghighatshoar-15a/</link>
      <pubDate>Fri, 03 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/haghighatshoar-15a/</guid>
      <description>Abstract LDPC codes are typically decoded by running a synchronous message passing algorithm over the corresponding bipartite factor graph (made of variable and check nodes). More specifically, each synchronous round consists of 1) updating all variable nodes based on the information received from the check nodes in the previous round, and then 2) updating all the check nodes based on the information sent from variable nodes in the current round. However, in many applications, ranging from message passing in neural networks to hardware implementation of LDPC codes, assuming that all messages are sent and received at the same time is far from realistic.</description>
    </item>
    
    <item>
      <title>Non-Monotone Adaptive Submodular Maximization.</title>
      <link>/publications/2015/gotovos-15a/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/gotovos-15a/</guid>
      <description>Abstract A wide range of AI problems, such as sensor placement, active learning, and network influence maximization, require sequentially selecting elements from a large set with the goal of optimizing the utility of the selected subset. Moreover, each element that is picked may provide stochastic feedback, which can be used to make smarter decisions about future selections. Finding efficient policies for this general class of adaptive optimization problems can be extremely hard.</description>
    </item>
    
    <item>
      <title>Near-Optimally Teaching the Crowd to Classify.</title>
      <link>/publications/2014/singla-14a/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/singla-14a/</guid>
      <description>Abstract How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners.</description>
    </item>
    
    <item>
      <title>Near Optimal Bayesian Active Learning for Decision Making</title>
      <link>/publications/2014/javdani-14a/</link>
      <pubDate>Wed, 03 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/javdani-14a/</guid>
      <description>Abstract How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations.</description>
    </item>
    
    <item>
      <title>Streaming submodular maximization: massive data summarization on the fly.</title>
      <link>/publications/2014/badandidiyuru-14a/</link>
      <pubDate>Sun, 03 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/badandidiyuru-14a/</guid>
      <description>Abstract How can one summarize a massive data set &amp;ldquo;on the fly&amp;rdquo;, i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of &amp;ldquo;representativeness&amp;rdquo; satisfy submodularity, an intuitive notion of diminishing returns.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization: Identifying Representative Elements in Massive Data.</title>
      <link>/publications/2013/baharan-13a/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/baharan-13a/</guid>
      <description>Abstract Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable, representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Noise-Enhanced Associative Memories</title>
      <link>/publications/2013/karbasi-13d/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13d/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small.</description>
    </item>
    
    <item>
      <title>Iterative Learning and Denoising in Convolutional Neural Associative Memories.</title>
      <link>/publications/2013/karbasi-13b/</link>
      <pubDate>Thu, 03 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once.</description>
    </item>
    
    <item>
      <title>Constrained Binary Identification Problem</title>
      <link>/publications/2013/karbasi-13e/</link>
      <pubDate>Mon, 03 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13e/</guid>
      <description>Abstract We consider the problem of building a binary decision tree, to locate an object within a set by way of the least number of membership queries. This problem is equivalent to the &amp;ldquo;20 questions game&amp;rdquo; of information theory and is closely related to lossless source compression. If any query is admissible, Huffman coding is optimal with close to H[P] questions on average, the entropy of the prior distribution P over objects.</description>
    </item>
    
    <item>
      <title>Coupled neural associative memories.</title>
      <link>/publications/2013/karbasi-13c/</link>
      <pubDate>Fri, 03 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13c/</guid>
      <description>Abstract We propose a novel architecture to design a neural associative memory that is capable of learning a large number of patterns and recalling them later in presence of noise. It is based on dividing the neurons into local clusters and parallel plains, an architecture that is similar to the visual cortex of macaque brain. The common features of our proposed model with those of spatially-coupled codes enable us to show that the performance of such networks in eliminating noise is drastically better than the previous approaches while maintaining the ability of learning an exponentially large number of patterns.</description>
    </item>
    
    <item>
      <title>Comparison-Based Learning with Rank Nets</title>
      <link>/publications/2012/karbasi-12a/</link>
      <pubDate>Wed, 03 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12a/</guid>
      <description>Abstract We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adap- tive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the tar- get in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum.</description>
    </item>
    
    <item>
      <title>Multi-level error-resilient neural networks</title>
      <link>/publications/2012/salavati-12a/</link>
      <pubDate>Thu, 03 May 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/salavati-12a/</guid>
      <description>Abstract The problem of neural network association is to retrieve a previously memorized pattern from its noisy version using a network of neurons. An ideal neural network should include three components simultaneously: a learning algorithm, a large pattern retrieval capacity and resilience against noise. Prior works in this area usually improve one or two aspects at the cost of the third. Our work takes a step forward in closing this gap.</description>
    </item>
    
    <item>
      <title>Sequential group testing with graph constraints</title>
      <link>/publications/2012/karbasi-12c/</link>
      <pubDate>Tue, 03 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12c/</guid>
      <description>Abstract In conventional group testing, the goal is to detect a small subset of defecting items D in a large population N by grouping arbitrary subset of N into different pools. The result of each group test T is a binary output depending on whether the group contains a defective item or not. The main challenge is to minimize the number of pools required to identify the set D. Motivated by applications in network monitoring and infection propagation, we consider the problem of group testing with graph constraints.</description>
    </item>
    
    <item>
      <title>Hot or not: Interactive content search using comparisons</title>
      <link>/publications/2012/karbasi-12b/</link>
      <pubDate>Sat, 03 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/karbasi-12b/</guid>
      <description>Abstract In interactive content search through comparisons, a user searching for a target object in a database is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selections. This process is repeated until the target is included in the list presented, at which point the search terminates. We study this problem under the scenario of heterogeneous demand, where target objects are selected from a non-uniform probability distribution.</description>
    </item>
    
    <item>
      <title>Calibration in circular ultrasound tomography devices.</title>
      <link>/publications/2011/parhizkar-11a/</link>
      <pubDate>Sun, 03 Jul 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/parhizkar-11a/</guid>
      <description>Abstract We consider the position calibration problem in circular tomography devices, where sensors deviate from a perfect circle. We introduce a new method of calibration based on the time-of-fiight measurements between sensors when the enclosed medium is homogeneous. Bounds on the reconstruction errors are proven and results of simulations mimicking a scanning device are presented.</description>
    </item>
    
    <item>
      <title>Content Search through Comparisons.</title>
      <link>/publications/2011/karbasi-11a/</link>
      <pubDate>Thu, 03 Mar 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11a/</guid>
      <description>Abstract We study the problem of navigating through a database of similar objects using comparisons under heterogeneous demand, a problem closely related to small-world network design. We show that, under heterogeneous demand, the small-world network design problem is NP-hard. Given the above negative result, we propose a novel mechanism for small-world network design and provide an upper bound on its performance under heterogeneous demand. The above mechanism has a natural equivalent in the context of content search through comparisons, again under heterogeneous demand; we use this to establish both upper and lower bounds on content search through comparisons.</description>
    </item>
    
    <item>
      <title>Compression with graphical constraints: An interactive browser</title>
      <link>/publications/2011/karbasi-11b/</link>
      <pubDate>Thu, 03 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/karbasi-11b/</guid>
      <description>Abstract We study the problem of searching for a given element in a set of objects using a membership oracle. The membership oracle, given a subset of objects A, and a target object t, determines whether A contains t or not. The goal is to find the target object with the minimum number of questions asked from the oracle. This problem is known to be strongly related to lossless source compression.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2010/cheraghchi-10a/</link>
      <pubDate>Mon, 03 May 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/cheraghchi-10a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>From centralized to distributed sensor localization</title>
      <link>/publications/2010/karbasi-10a/</link>
      <pubDate>Sat, 03 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10a/</guid>
      <description>Abstract In this work we consider the problem of sensor network localization when only the connectivity information is available. More specifically, we compare the performance of the centralized algorithm MDS-MAP with its distributed version HOP-TERRAIN. We show that both algorithms are able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r.</description>
    </item>
    
    <item>
      <title>Distributed sensor network localization from local connectivity: performance analysis for the HOP-TERRAIN algorithm</title>
      <link>/publications/2010/karbasi-10b/</link>
      <pubDate>Wed, 03 Mar 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/karbasi-10b/</guid>
      <description>Abstract This paper addresses the problem of determining the node locations in ad-hoc sensor networks when only connectivity information is available. In previous work, we showed that the localization algorithm MDS-MAP proposed by Y. Shang et al. is able to localize sensors up to a bounded error decreasing at a rate inversely proportional to the radio range r. The main limitation of MDS-MAP is the assumption that the available connectivity information is processed in a centralized way.</description>
    </item>
    
    <item>
      <title>Calibration for Ultrasound Breast Tomography Using Matrix Completion</title>
      <link>/publications/2010/parhizkar-11a/</link>
      <pubDate>Wed, 03 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>/publications/2010/parhizkar-11a/</guid>
      <description>Abstract We study the calibration process in circular ultrasound tomography devices where the sensor positions deviate from the circumference of a perfect circle. This problem arises in a variety of applications in signal processing ranging from breast imaging to sensor network localization. We introduce a novel method of calibration/localization based on the time-of-flight (ToF) measurements between sensors when the enclosed medium is homogeneous. In the presence of all the pairwise ToFs, one can easily estimate the sensor positions using multi-dimensional scaling (MDS) method.</description>
    </item>
    
    <item>
      <title>Support recovery in compressed sensing: An estimation theoretic approach</title>
      <link>/publications/2009/karbasi-09a/</link>
      <pubDate>Tue, 03 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/karbasi-09a/</guid>
      <description>Abstract Compressed sensing (CS) deals with the reconstruction of sparse signals from a small number of linear measurements. One of the main challenges in CS is to find the support of a sparse signal from a set of noisy observations. In the CS literature, several information-theoretic bounds on the scaling law of the required number of measurements for exact support recovery have been derived, where the focus is mainly on random measurement matrices.</description>
    </item>
    
    <item>
      <title>Compressed Sensing with Probabilistic Measurements: A Group Testing Solution</title>
      <link>/publications/2009/cheraghchi-09a/</link>
      <pubDate>Tue, 03 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>/publications/2009/cheraghchi-09a/</guid>
      <description>Abstract Detection of defective members of large populations has been widely studied in the statistics community under the name &amp;ldquo;group testing&amp;rdquo;, a problem which dates back to World War II when it was suggested for syphilis screening. There the main interest is to identify a small number of infected people among a large population using collective samples. In viral epidemics, one way to acquire collective samples is by sending agents inside the population.</description>
    </item>
    
    <item>
      <title>A new DOA estimation method using a circular microphone array</title>
      <link>/publications/2007/karbasi-07a/</link>
      <pubDate>Wed, 03 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>/publications/2007/karbasi-07a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method based on circular microphone array. For an arbitrary number of microphones, it is analytically shown that DOA estimation reduces to an efficient non-linear optimization problem. Simulation results demonstrate that deviation of the estimation error for 20 and 10 dB SNR is smaller than 0.7 degree which is comparable to high resolution DOA estimation methods. A larger number of microphones provide a more omni-directional spatial resolution.</description>
    </item>
    
    <item>
      <title>A DOA estimation method for an arbitrary triangular microphone arrangement.</title>
      <link>/publications/2006/karbasi-06a/</link>
      <pubDate>Tue, 03 Jan 2006 00:00:00 +0000</pubDate>
      
      <guid>/publications/2006/karbasi-06a/</guid>
      <description>Abstract This paper proposes a new DOA (direction of arrival) estimation method for an arbitrary triangular microphone arrangement. Using the phase rotation factors for the crosscorrelations between the adjacent-microphone signals, a general form of the integrated cross spectrum is derived. DOA estimation is reduced to a non-linear optimization problem of the general integrated cross spectrum. It is shown that a conventional DOA estimation for the equilateral triangular microphone arrangement is a special case of the proposed method.</description>
    </item>
    
    <item>
      <title>How Do You Want Your Greedy: Simultaneous or Repeated?</title>
      <link>/publications/2023/jmlr-2023/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/publications/2023/jmlr-2023/</guid>
      <description>Abstract We present SimultaneousGreedys, a deterministic algorithm for constrained submodular maximization. At a high level, the algorithm maintains $l$ solutions and greedily updates them in a simultaneous fashion. SimultaneousGreedys achieves the tightest known approximation guarantees for both $k$-extendible systems and the more general $k$-systems, which are $(k+1)^2/k=k+O(1)$ and $(1+\sqrt{k+2})^2=k+O(\sqrt{k})$, respectively. We also improve the analysis of RepeatedGreedy, showing that it achieves an approximation $\sqrt{\sqrt{k}+O(k)}$ ratio for $k$-systems when allowed to run for $O(k)$ iterations, an improvement in both the runtime and approximation over previous analyses.</description>
    </item>
    
    <item>
      <title>Cross Atlas Remapping via Optimal Transport (CAROT): Creating connectomes for any atlas when raw data is not available</title>
      <link>/publications/2022/nature-2022/</link>
      <pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/publications/2022/nature-2022/</guid>
      <description>Abstract Whether using large-scale projects&amp;mdash;like the Human Connectome Project (HCP), the Adolescent Brain Cognitive Development (ABCD) study, Healthy Brain Network (HBN), and the UK Biobank&amp;mdash;or pooling together several smaller studies, open-source, publicly available datasets allow for unpresented sample sizes and promote generalization efforts. Overall, releasing preprocessing data can enhance participant privacy, democratize science, and lead to unique scientific discoveries. But releasing preprocessed data also limits the choices available to the end-user.</description>
    </item>
    
    <item>
      <title>The Power of Subsampling in Submodular Maximization</title>
      <link>/publications/2021/chris-21/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/2021/chris-21/</guid>
      <description>Abstract We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a (p+2+o(1))-approximation for maximizing a submodular function subject to a p-extendible system using O(n+nk/p) evaluation and feasibility queries, where k is the size of the largest feasible set.</description>
    </item>
    
    <item>
      <title>Individualized functional networks reconfigure with cognitive state</title>
      <link>/publications/2020/mehraveh-2020a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020a/</guid>
      <description>Abstract There is extensive evidence that functional organization of the human brain varies dynamically as the brain switches between task demands, or cognitive states. This functional organization also varies across subjects, even when engaged in similar tasks. To date, the functional network organization of the brain has been considered static. In this work, we use fMRI data obtained across multiple cognitive states (task-evoked and rest conditions) and across multiple subjects, to measure state- and subject-specific functional network parcellation (the assignment of nodes to networks).</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization</title>
      <link>/publications/2020/mokhtaric-20a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mokhtaric-20a/</guid>
      <description>Abstract This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is costly. Instead, stochastic conditional gradient methods are proposed as an alternative solution relying on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction.</description>
    </item>
    
    <item>
      <title>Stochastic Conditional Gradient&#43;&#43;</title>
      <link>/publications/2020/hassani-2019a/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/hassani-2019a/</guid>
      <description>Abstract In this paper, we consider the general non-oblivious stochastic optimization where the underlying stochasticity may change during the optimization procedure and depends on the point at which the function is evaluated. We develop Stochastic Frank-Wolfe++ (SFW++), an efficient variant of the conditional gradient method for minimizing a smooth non-convex function subject to a convex body constraint. We show that SFW++ converges to an ϵ-first order stationary point by using $O(1/ϵ^3)$ stochastic gradients.</description>
    </item>
    
    <item>
      <title>Submodularity in Action: From Machine Learning to Signal Processing Applications</title>
      <link>/publications/2020/karbasi-2020d/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/karbasi-2020d/</guid>
      <description>Abstract Submodularity is a discrete domain functional prop- erty that can be interpreted as mimicking the role of the well- known convexity/concavity properties in the continuous domain. Submodular functions exhibit strong structure that lead to efficient optimization algorithms with provable near-optimality guarantees. These characteristics, namely, efficiency and provable performance bounds, are of particular interest for signal process- ing (SP) and machine learning (ML) practitioners as a variety of discrete optimization problems are encountered in a wide range of applications.</description>
    </item>
    
    <item>
      <title>There is no single functional atlas even for a single individual: Functional parcel definitions change with task</title>
      <link>/publications/2020/mehraveh-2020b/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/2020/mehraveh-2020b/</guid>
      <description>Abstract The goal of human brain mapping has long been to delineate the functional subunits in the brain and elucidate the functional role of each of these brain regions. Recent work has focused on whole-brain parcellation of functional Magnetic Resonance Imaging (fMRI) data to identify these subunits and create a functional atlas. Functional connectivity approaches to understand the brain at the network level require such an atlas to assess connections between parcels and extract network properties.</description>
    </item>
    
    <item>
      <title> Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences</title>
      <link>/publications/thesis/marko-2020.md/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/marko-2020.md/</guid>
      <description>Student: Marko Mitrovic
Dissertation Title: Modern Challenges for Machine Learning Applications of Submodularity: Privacy, Scalability, and Sequences
Date: Thursday, March 5, 2020 Time: 4:00 PM Location: Room 335, 3rd floor, 17 Hillhouse Avenue
Advisor: Amin Karbasi
Other committee members:
Dan Spielman Dragomir Radev Yaron Singer (Harvard) Abstract :In a nutshell, submodularity covers the class of all problems that exhibit some form of diminishing returns. From a theoretical perspective, this notion of diminishing returns is extremely useful as the resulting mathematical properties allow for provably efficient optimization.</description>
    </item>
    
    <item>
      <title>An exemplar-based approach to individualized parcellation reveals the need for sex specific functional networks.</title>
      <link>/publications/2018/mehraveh-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/mehraveh-2018a/</guid>
      <description>Abstract Recent work with functional connectivity data has led to significant progress in understanding the functional organization of the brain. While the majority of the literature has focused on group-level parcellation approaches, there is ample evidence that the brain varies in both structure and function across individuals. In this work, we introduce a parcellation technique that incorporates delineation of functional networks both at the individual- and group-level. The proposed technique deploys the notion of “submodularity” to jointly parcellate the cerebral cortex while establishing an inclusive correspondence between the individualized functional networks.</description>
    </item>
    
    <item>
      <title>Learning neural connectivity from firing activity: efficient algorithms with provable guarantees on topology.</title>
      <link>/publications/2018/karbasi-2018a/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publications/2018/karbasi-2018a/</guid>
      <description>Abstract The connectivity of a neuronal network has a major effect on its functionality and role. It is generally believed that the complex network structure of the brain provides a physiological basis for information processing. Therefore, identifying the network’s topology has received a lot of attentions in neuroscience and has been the center of many research initiatives such as Human Connectome Project. Nevertheless, direct and invasive approaches that slice and observe the neural tissue have proven to be time consuming, complex and costly.</description>
    </item>
    
    <item>
      <title>Distributed Submodular Maximization</title>
      <link>/publications/2016/baharan-16a/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16a/</guid>
      <description>Abstract Many large-scale machine learning problems–clustering, non-parametric learning, kernel machines, etc.–require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion.</description>
    </item>
    
    <item>
      <title>Fast Distributed Submodular Cover: Public-Private Data Summarization.</title>
      <link>/publications/2016/baharan-16c/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16c/</guid>
      <description>Abstract In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy.</description>
    </item>
    
    <item>
      <title>Fast Constrained Submodular Maximization: Personalized Data Summarization.</title>
      <link>/publications/2016/baharan-16b/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/publications/2016/baharan-16b/</guid>
      <description>Abstract Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains.</description>
    </item>
    
    <item>
      <title>From Small-World Networks to Comparison-Based Search.</title>
      <link>/publications/2015/karbasi-15a/</link>
      <pubDate>Thu, 03 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/publications/2015/karbasi-15a/</guid>
      <description>Abstract The problem of content search through comparisons has recently received considerable attention. In short, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from a small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Noise Facilitation in Associative Memories of Exponential Capacity</title>
      <link>/publications/2014/karbasi-14a/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14a/</guid>
      <description>Abstract Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns that satisfy certain subspace constraints. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in brain regions thought to operate associatively, such as hippocampus and olfactory cortex. Here we consider associative memories with boundedly noisy internal computations and analytically characterize performance.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance.</title>
      <link>/publications/2014/karbasi-14b/</link>
      <pubDate>Fri, 03 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14b/</guid>
      <description>Abstract The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons. An ideal network should have the ability to 1) learn a set of patterns as they arrive, 2) retrieve the correct patterns from noisy queries, and 3) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries. The majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity.</description>
    </item>
    
    <item>
      <title>Calibration Using Matrix Completion With Application to Ultrasound Tomography.</title>
      <link>/publications/2013/parhizkar-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/parhizkar-13a/</guid>
      <description>Abstract We study the application of matrix completion in the process of calibrating physical devices. In particular we propose an algorithm together with reconstruction bounds for calibrating circular ultrasound tomography devices. We use the time-of-flight (ToF) measurements between sensor pairs in a homogeneous medium to calibrate the system. The calibration process consists of a low-rank matrix completion algorithm to de-noise and estimate random and structured missing ToFs, and the classic multi-dimensional scaling method to estimate the sensor positions from the ToF measurements.</description>
    </item>
    
    <item>
      <title>Robust Localization From Incomplete Local Information</title>
      <link>/publications/2013/karbasi-13a/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13a/</guid>
      <description>Abstract We consider the problem of localizing wireless devices in an ad hoc network embedded in a d-dimensional Euclidean space. Obtaining a good estimate of where wireless devices are located is crucial in wireless network applications including environment monitoring, geographic routing, and topology control. When the positions of the devices are unknown and only local distance information is given, we need to infer the positions from these local distance measurements. This problem is particularly challenging when we only have access to measurements that have limited accuracy and are incomplete.</description>
    </item>
    
    <item>
      <title>Graph-Constrained Group Testing</title>
      <link>/publications/2012/cheraghchi-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/cheraghchi-12a/</guid>
      <description>Abstract Nonadaptive group testing involves grouping arbitrary subsets of n items into different pools. Each pool is then tested and defective items are identified. A fundamental question involves minimizing the number of pools required to identify at most d defective items. Motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. Unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph.</description>
    </item>
    
    <item>
      <title>Low-Rank Matrix Approximation Using Point-Wise Operators</title>
      <link>/publications/2012/amini-12a/</link>
      <pubDate>Mon, 03 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/publications/2012/amini-12a/</guid>
      <description>Abstract The problem of extracting low-dimensional structure from high-dimensional data arises in many applications such as machine learning, statistical pattern recognition, wireless sensor networks, and data compression. If the data is restricted to a lower dimensional subspace, then simple algorithms using linear projections can find the subspace and consequently estimate its dimensionality. However, if the data lies on a low-dimensional but nonlinear space (e.g., manifolds), then its structure may be highly nonlinear and, hence, linear methods are doomed to fail.</description>
    </item>
    
    <item>
      <title>Group Testing With Probabilistic Tests: Theory, Design and Application.</title>
      <link>/publications/2011/cheraghchi-11a/</link>
      <pubDate>Sat, 03 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/publications/2011/cheraghchi-11a/</guid>
      <description>Abstract Identification of defective members of large populations has been widely studied in the statistics community under the name of group testing. It involves grouping subsets of items into different pools and detecting defective members based on the set of test results obtained for each pool. In a classical noiseless group testing setup, it is assumed that the sampling procedure is fully known to the reconstruction algorithm, in the sense that the existence of a defective member in a pool results in the test outcome of that pool to be positive.</description>
    </item>
    
    <item>
      <title>Allocating tasks to machines in computing clusters</title>
      <link>/publications/2014/karbasi-14c/</link>
      <pubDate>Mon, 03 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/publications/2014/karbasi-14c/</guid>
      <description>Abstract Allocating tasks to machines in computing clusters is described. In an embodiment a set of tasks associated with a job are received at a scheduler. In an embodiment an index can be computed for each combination of tasks and processors and stored in a lookup table. In an example the index may be include an indication of the preference for the task to be processed on a particular processor, an indication of a waiting time for the task to be processed and an indication of how other tasks being processed in the computing cluster may be penalized by assigning a task to a particular processor.</description>
    </item>
    
    <item>
      <title>Hot or Not: Interactive Content Search Using Comparisons</title>
      <link>/publications/2013/karbasi-13f/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/2013/karbasi-13f/</guid>
      <description>Abstract In comparison-based active learning, a user searching for a target object navigates through a database in the following manner. The user is asked to select the object most similar to her target from small list of objects. A new object list is then presented to the user based on her earlier selection. This process is repeated until the target is included in the list presented, at which point the search terminates.</description>
    </item>
    
    <item>
      <title>Scalable Projection-Free Optimization</title>
      <link>/publications/thesis/mingrui-21.md/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mingrui-21.md/</guid>
      <description>Adviserr: Amin Karbasi
Abstract: As a projection-free algorithm, Frank-Wolfe (FW) method, also known as conditional gradient, has recently received considerable attention in the machine learning community. In this dissertation1, we study several topics on the FW variants for scalable projection-free optimization. We first propose 1-SFW, the first projection-free method that requires only one sample per iteration to update the optimization variable and yet achieves the best known complexity bounds for convex, non-convex, and monotone DR-submodular settings.</description>
    </item>
    
    <item>
      <title>Online Optimization: Convex and Submodular Functions</title>
      <link>/publications/thesis/lin-2020.md/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/lin-2020.md/</guid>
      <description>The final doctoral examination for Lin Chen will take place on Wednesday, March 11th, at 2:00pm in HLH17, Room 335.
The title of the thesis is: Online Optimization: Convex and Submodular Functions
Advisor: Amin Karbasi
Members of the Committee are:
Professor Negahban Professor Spielman Abstract: In the first part, we study switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action. While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it.</description>
    </item>
    
    <item>
      <title>Individualized and Task-Specific Functional Brain Mapping</title>
      <link>/publications/thesis/mehraveh-19.md/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/mehraveh-19.md/</guid>
      <description>Mehraveh Salehi Graduated with her PhD thesis entitled “Individualized and Task-Specific Human Brain Mapping”.
The final doctoral examination for Mehraveh Salehi took place on Tuesday, September 3rd, at 11:00am at YINS, 17 Hillhouse, 3rd floor.
The title of the thesis is: Individualized and Task-Specific Functional Brain Mapping
Adviserr:
Amin Karbasi Todd Constable Members of the Committee are:
Professor Papademetris Professor Tassiulas Professor Jeff Bilmes Abstract: Understanding the human brain, with its remarkable ability to control higher thought, behavior, and memory, remains one of the greatest intellectual challenges in all of science.</description>
    </item>
    
    <item>
      <title>Graph-Based Information Processing: Scaling Laws and Applications</title>
      <link>/publications/thesis/karbasi-13e/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/publications/thesis/karbasi-13e/</guid>
      <description>Abstract We live in a world characterized by massive information transfer and real-time communication. The demand for efficient yet low-complexity algorithms is widespread across different fields, including machine learning, signal processing and communications. Most of the problems that we encounter across these disciplines involves a large number of modules interacting with each other. It is therefore natural to represent these interactions and the ow of information between the modules in terms of a graph.</description>
    </item>
    
  </channel>
</rss>
